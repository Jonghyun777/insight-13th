{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea57ed7-83c8-440e-b680-30647c9e64d0",
   "metadata": {},
   "source": [
    "# 1. 군집화 (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64270f91-1b72-468c-805b-7b0b3a4da881",
   "metadata": {},
   "source": [
    "## 1) 머신러닝: 비지도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2373f75-b106-4e51-b620-6e5ff7d542bd",
   "metadata": {},
   "source": [
    "정답이 없는 데이터를 이용하는 머신러닝 기법\n",
    "- 데이터 자체에 내재된 구조를 학습\n",
    "- 패턴이 알려지지 않았거나, 계속해서 변화하는 문제 해결에 유용\n",
    "- 구체적, 좁은 범위 -> 지도 학습\n",
    "- 열린 문제, 지식 일반화 -> 비지도 학습\n",
    "- 모델 성능의 명확한 측정 제한 (정답 레이블 부재)\n",
    "- 표현 학습(representation learning or 피처 학습)을 통해 데이터셋 고유 패턴 식별 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea0e2a-25ba-47ed-b1d5-624e791b26c6",
   "metadata": {},
   "source": [
    "## 2) 군집화란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63976ce8-121e-40e6-a87a-89c5e0f93a3e",
   "metadata": {},
   "source": [
    "데이터를 비슷한 특성을 가진 그룹(cluster, 군집)으로 나누는 비지도 학습 기법\n",
    "- 각 그룹의 구성을 통해, 전체 데이터 구조 파악\n",
    "- '유사성'을 기반으로 개체를 그룹화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3aca5-c75d-4cd1-9495-59d880af99d8",
   "metadata": {},
   "source": [
    "군집화의 목표\n",
    "1. 응집도(Cohension) 최대화\n",
    "   - 같은 군집에 속하는 데이터끼리는 최대한 비슷하도록 함\n",
    "2. 분리도(Separation) 최대화\n",
    "   - 서로 다른 군집은 최대한 분리되도록 함\n",
    "\n",
    "데이터간 유사성 최대한 유지 + 서로 다른 그룹은 구분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef559da0-bfab-42d0-927c-5e078e0036cf",
   "metadata": {},
   "source": [
    "## 3) 군집화 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119cee7-4877-422d-a3e5-135663ed2def",
   "metadata": {},
   "source": [
    "1. 피처 선택/추출\n",
    "2. 군집화 알고리즘 선택\n",
    "3. 군집 유효성 검증\n",
    "4. 결과 해석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff639cba-cd6b-4fcf-9513-22116b84fc88",
   "metadata": {},
   "source": [
    "주요 고려 사항\n",
    "1. 변수 유형 이해 (사용되는 방법론 결정)\n",
    "   - 연속형/범주형\n",
    "   - 변수 개수와 특징\n",
    "2. 거리 (또는 유사도) 정의와 측정\n",
    "   - 거리(distance) or 유사도(similarity) 측정 방식 중요\n",
    "   - 회귀 분석에서는 변수 자체가 중요했다면, 군집 분석에서는 거리를 어떻게 정의하고 측정할 것인지가 중요\n",
    "3. 차원 축소\n",
    "   - 유사한 변수들을 묶어서 차원 축소\n",
    "4. 군집화는 one-shot 프로세스가 아님\n",
    "   - 반복적 시도 요구\n",
    "5. 피처와 군집화 스키마 선택 시의 명확한 기준 없음\n",
    "   - 적합한 평가 기준 선정도 하나의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff64d1-df07-42d4-a17d-e3b160928c4e",
   "metadata": {},
   "source": [
    "# 2. 군집화 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4c1e5-5ef2-4082-aad1-a00b633c12a2",
   "metadata": {},
   "source": [
    "## 1) 계층적 군집화 (Hierarchical Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dea26d-159b-4f66-9e4e-f4d1ba738b3d",
   "metadata": {},
   "source": [
    "- 데이터 간의 유사성을 기반으로 트리 구조 형성해,\\\n",
    "  상향식 or 하향식으로 군집을 형성해 나가는 방법\n",
    "\n",
    "- 덴드로그램 (Dendrogram)\n",
    "    - 가까운 두 개체/군집을 점차적으로 병합해 가는 과정을 시각적으로 표현한 트리 구조\n",
    "    - 해석: 수직축(높이) = 두 군집이 병합될 때의 군집 간 거리(유사성)\n",
    "        - 두 그룹을 연결하는 수직축이 짧을수록, 유사도가 높음\n",
    "- 군집의 개수를 사전에 설정하지 않음\n",
    "    - 클러스터링 종료 후, 원하는 군집 개수 선택\n",
    "- 계층적 군집화 활용\n",
    "    - 소규모 데이터에 대해 군집 수 탐색\n",
    "    - 클러스터링의 전반적 구조를 시각적으로 분석\n",
    "    - 계산량이 많기 때문에, 대규모 데이터에 적용 비추천"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee8a02-c82a-4288-9534-36f8a6f1298c",
   "metadata": {},
   "source": [
    "1. 응집형 계층적 군집화 (Agglomerative Hierarchical Clustering, 병합 클러스터링)\n",
    "   - 상향식 트리 구조\n",
    "   - 작은 군집을 병합해 더 큰 군집 형성\n",
    "   - 각 샘플이 독립적인 클러스터로 시작 -. 하나의 클러스터만 남을 때까지 클러스터 병합\n",
    "   - 군집 간의 거리 정의에 따라 알고리즘 변화 (가까운 군집 간 병합 시 - '가깝다'의 기준)\n",
    "2. 분리형 계층적 군집화 (Divisive Hierarchical Clustering, 분할 클러스터링)\n",
    "   - 하향식 트리 구조\n",
    "   - 하나의 군집을 분할해가며 여러 군집 형성\n",
    "   - 전체 샘플을 포함하는 하나의 클러스터에서 시작 -> 유사성이 낮은 데이터들을 더 작은 클러스터로 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9c978-3254-4180-963c-008bd22f8ed1",
   "metadata": {},
   "source": [
    "응집형 계층적 군집화의 여러 가지 정의 알고리즘\n",
    "  1. 단일 연결법 (Single Linkage, 최단 연결법)\n",
    "     - 클러스터 쌍에서 가장 비슷한 샘플 간 거리 계산\\\n",
    "       -> 이후, 이 거리가 가장 작은 두 클러스터 병합\n",
    "     - 고립된 군집을 찾는데 중점\n",
    "     - 이상치에 민감 (Not Robust)\n",
    "       - 이상치와 가까이 있는 점은 이상치의 영향에 따른 군집 형성\n",
    "  2. 완전 연결법 (Complete Linkage, 최장 연결법)\n",
    "     - 클러스터 쌍에서 가장 비슷하지 않은 샘플 간 거리 계산\\\n",
    "       -> 이후, 가장 유사성이 큰 군집으로 병합\n",
    "     - 군집들 간 내부 응집성에 중점\n",
    "     - 이상치에 민감\n",
    "  3. 평균 연결법 (Average Linkage)\n",
    "     - 모든 항목에 대한 거리 평균 계산\n",
    "     - 이상치에 1,2번 대비 상대적으로 덜 민감\n",
    "     - 계산량이 불필요하게 과다해 질 수 있음\n",
    "  4. 중심 연결법 (Centroid Method)\n",
    "     - 두 군집의 중심 간 거리 계산\n",
    "       -> 두 군집이 결합될 때 새로운 군집의 중심은 가중평균을 통해 구함\n",
    "     - 군집들의 중심 계산에 시간이 오래 걸림\n",
    "  5. 중앙 연결법 (Median)\n",
    "     - 군집 간 거리를 군집 내 모든 샘플의 중앙값으로 정의\n",
    "     - 이상치에 덜 민감\n",
    "     - 기하학적 구조 파악이 어려움\n",
    "  6. Ward 연결법 (Ward's Procedure)\n",
    "     - 군집 병합 후, 군집 내 SSE(오차제곱합)의 증가분 최소인 것을 선택\n",
    "     - 병합된 군집의 SSE 증가량이 가장 작아지는 방향으로 군집 형성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbdefc-6656-4a6f-9875-4848cf73085c",
   "metadata": {},
   "source": [
    "완전 연결 방식을 사용한 응집형 계층 군집화 과정\n",
    "1. 모든 샘플의 거리 행렬 계산\n",
    "2. 모든 데이터 포인트가 단일 클러스터로 시작\n",
    "3. 가장 거리가 먼 샘플 사이 거리에 기초해, 가장 가까운 두 클러스터 병합\n",
    "4. 유사도 행렬 업데이트\n",
    "5. 하나의 클러스터 남을 때까지 2~4번 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59809538-cec9-42b4-9266-bdc66708fcac",
   "metadata": {},
   "source": [
    "## 2) k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e204eff-25dd-4b95-9a10-2cf4fe726ed2",
   "metadata": {},
   "source": [
    "데이터를 정해진 개수(k)의 그룹으로 나누되, 각 그룹의 중심점(centroid)과의 거리가 가장 가까운 데이터끼리 묶는 군집화 알고리즘\n",
    "- 군집화에서 가장 일반적으로 사용\n",
    "- 연속적인 특성을 갖는 데이터 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a40cf3-51f1-4c37-9386-ea6a52f573c9",
   "metadata": {},
   "source": [
    "- k-means 알고리즘의 주요 단계\n",
    "  1. 데이터 표본 중 랜덤하게 k개의 중심점(centroid)를 초기 클러스터 중심으로 설정\n",
    "  2. 각 표본들을 가장 가까운 중심점에 할당\n",
    "     - 유클리디안 거리 제곱 사용\n",
    "  3. 각 클러스터에 할당된 표본들의 데이터 평균 계산 + 중심점 이동\n",
    "  4. 클러스터 할당이 변하지 않거나, 사용자가 지정한 허용 오차 or 최대 반복 횟수(max_iter)에 도달할 때까지 2,3번 반복\n",
    "\n",
    "- k-means 알고리즘의 목표\n",
    "    - 클러스터 내 제곱 오차합(SSE) 최소화\n",
    "- k-means 알고리즘의 장점\n",
    "    1. 원형 클러스터를 구분하는 데 효과적\n",
    "    2. 직관적, 구현이 쉬움\n",
    "    3. 대용량 데이터에도 적용 가능\n",
    "- k-means 알고리즘 단점\n",
    "    1. 클러스터 개수(k) 직접 지정 필요 (하이퍼파라미터)\n",
    "       - 적절한 k값을 고르지 않으면, 모델 성능 하락\n",
    "         - 엘보우 방법(elbow method), 실루엣 계수(silhouette coefficient) 등 활용 가능\n",
    "    2. 초기 centroid 값에 민감\n",
    "    3. 아웃라이어에 민감\n",
    "       - 평균 중심의 군집 구성\n",
    "         - 아웃라이어가 중심 왜곡시킬 수있음\n",
    "    4. 기하학적 모양의 군집 파악 어려움\n",
    "       - 문제 해결을 위해 K-Median(중앙값), K-Medoids(데이터 포인트 하나 선택), Mean Shift(밀도 높은 곳으로 중심점 이동) 등의 알고리즘 고려 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470e13f-2f7a-4582-8bf7-04473570f4e6",
   "metadata": {},
   "source": [
    "k-means++ 알고리즘\n",
    "- 기존 k-means 알고리즘의 초기 중심점 랜덤 할당에서 비롯된 문제 해결 알고리즘\n",
    "- 초기 중심점들을 서로 멀리 떨어진 곳에 위치시키기\n",
    "  - 기본적 k-means 보다 일관되고 좋은 결과 도출 가능\n",
    " \n",
    "- 프로세스\n",
    "  1. 선택한 k개의 중심점을 저장할 빈 집합 M 초기화\n",
    "  2. 입력 샘플에서 첫 번째 중심점을 랜덤하게 선택해 M에 할당\n",
    "  3. M에 없는 각 샘플 x에 대해 M에 있는 중심점까지의 최소 제곱 거리 d 찾기\n",
    "  4. 가중치가 적용된 확률분포를 사용해 다음 중심점 랜덤하게 선택\n",
    "  5. k개의 중심점을 선택할 때까지 3,4번 반복\n",
    "  6. 기본 k-means 알고리즘 수행\n",
    "  - 기호, 식과 함께 이해하기 위해서 교육 자료 참고 필수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d2eee-3acd-4212-a81d-e2a0f50c3dc1",
   "metadata": {},
   "source": [
    "엘보우 방법 (elbow method)\n",
    "- 적절한 k 값을 찾기 위한 방법\n",
    "- 그래프를 활용해 최적 클러스터 개수 k를 추정\n",
    "  - 그래프의 기울기에 따른 최적점 확인\n",
    "- k가 증가하면, 왜곡이 줄어든다 (직관적 해석: 클러스터를 세분화 할수록 샘플이 할당된 중심점에 더 가까워 짐)\n",
    "- 목표: 왜곡이 빠르게 감소하는 지점의 k값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c19dd-a3b6-42f9-9d80-e43347529624",
   "metadata": {},
   "source": [
    "## 3) DBSCAN (Density-Based-Clustering Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfa864-b441-416e-bebb-42bce4221b23",
   "metadata": {},
   "source": [
    "밀집도 기반 군집화\n",
    "- 데이터 밀도 분포를 기반으로 고밀도 영역을 클러스터로 인식\n",
    "- 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ed958-2a2d-4a03-b8b6-f8a1cf3fcfba",
   "metadata": {},
   "source": [
    "DBSCAN 알고리즘 주요 단계\n",
    "1. 다음 3가지 조건에 따라 각 샘플을 '핵심 샘플', '경계 샘플', '잡음 샘플' 중 하나에 할당\n",
    "   1. 핵심 샘플: 어떤 샘플의 특정 반경 ∊(엡실론)안에 있는 이웃 샘플이 지정된 개수(MinPts) 이상\n",
    "   2. 경계 샘플: ∊ 이내에 MinPts보다 이웃이 적지만 다른 핵심 샘플의 반경 ∊ 안에 있는 샘플\n",
    "   3. 잡음 샘플: 1,2 모두 해당하지 않는 모든 샘플\n",
    "2. 개별 핵심 샘플 또는 핵심 샘플의 그룹(∊ 이내에 있는 핵심 샘플 연결)을 클러스터로 형성\n",
    "3. 경계 샘플을 해당 핵심 샘플 클러스터에 할당 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea78c23-a7c9-4b77-bb74-295925eb9802",
   "metadata": {},
   "source": [
    "DBSCAN의 하이퍼파라미터\n",
    "1. eps\n",
    "   - 데이터 포인트 사이의 최대 거리\n",
    "2. min_samples\n",
    "   - 포인트가 군집이 되기 위해 eps 거리 내 포인트가 얼마나 많아야 하는지\n",
    "   - 해당 값이 증가하면, 군집 수 감소 (군집 기준 상향)\n",
    "     \n",
    "=> 특정 데이터 포인트 기준으로 반경 eps 내에 자신을 포함한 데이터 포인트 개수가 min_sample 이상일 때\\\n",
    "-> 하나의 군집으로 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59e08c-cf36-4a22-98b6-b00696d6848a",
   "metadata": {},
   "source": [
    "1. DBSCAN의 장점\n",
    "    1. 임의의 기하학적 분포를 갖는 데이터도 잘 처리 가능\n",
    "        - k-means처럼 클러스터 모향을 원형으로 가정 X\n",
    "    2. 이상치에 민감하지 않음\n",
    "       - Noise를 통한 Outlier 검출 가능\n",
    "    3. 모든 샘플을 클러스터에 할당하지 않음 (잡음 샘플 구분)\n",
    "       - k-means, 계층 군집과 다른 점\n",
    "\n",
    "2. DBSCAN의 단점\n",
    "\n",
    "    1. epsilon과 min_samples 설정에 많은 영향을 받음\n",
    "    2. 적절한 epsilon 값 조절의 어려움\n",
    "      - 데이터셋에 대한 도메인 지식 필요한 경우 많음\n",
    "    3. 연산량이 많아 속도가 느림\n",
    "    4. 다른 밀도 분포를 가진 데이터의 군집 분석을 잘 못함\n",
    "        - 밀도가 높은 곳에 집중하다 보면, 밀도가 낮은 곳의 데이터 군집화 못 함 (잡음 샘플로 구분)\n",
    "    5. 차원의 저주를 벗어나지 못함\n",
    "       - 모든 유클리안 거리를 활용하는 알고리즘이 갖는 문제\n",
    "       - 같은 양의 데이터가 고차원으로 이동할 수록, 데이터 포인트 사이의 거리가 늘어남\\\n",
    "         -> 데이터의 밀도가 낮아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd198b-8898-4ecc-bc8a-bb994f5f2101",
   "metadata": {},
   "source": [
    "## 4) Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ef695-3333-4004-9d6d-1711c21b266e",
   "metadata": {},
   "source": [
    "- 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 군집화 방법\n",
    "- 모델 기반 군집화 (Model-Based Clusterin) 방법 중 하나\n",
    "  - 데이터를 생성하는 통계적 모델 가정 후, 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화를 수행하는 접근 방식\n",
    "  - 데이터 생성 메커니즘을 모델링한다는 특징 존재\n",
    "- 각 군집을 확률 분포로 간주하고, 전체 데이터 분포를 이질적인 여러 개의 확률 분포 혼합으로 보고 모델링\n",
    "   - K-means와 같은 거리 기반 군집화와 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493aff2-9f07-49b5-83cd-3d99c704cd6a",
   "metadata": {},
   "source": [
    "GMM의 가정\n",
    "- 관측된 데이터 포인트들이 특정 가우시안 확률 분포에 의해 생성되었음\n",
    "- 전체 데이터셋에 여러 개의 (다변량) 가우시안 분포가 섞여있고, 개별 데이터는 우도(가능도)에 따라 K개의 가우시안 분포 중 한가지에 속함\n",
    "  \n",
    "-> 섞인 데이터 분포에서 각각의 가우시안 분포를 추출 + 각각의 분포에 기반해 군집화 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b79a04-ec5a-491e-906b-b8ba02ebc420",
   "metadata": {},
   "source": [
    "확률과 우도\n",
    "1. 확률 (Probability)\n",
    "   - 고정된 확률분포에서 어떤 관측값이 나타나는지에 대한 확률\n",
    "   - 확률 분포에서, 특정 점들 사이의 확률밀도함수 (면적, 적분)\n",
    "2. 우도 (Likelihood)\n",
    "   - 고정된 관측값이 어떠한 확률분포에서 어느정도 확률로 나타나는지에 대한 확률\n",
    "   - 확률 분포에서, 특정 x 값에 대한 y 값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd310771-ed01-4cfa-8f0d-36f249e884a2",
   "metadata": {},
   "source": [
    "GMM 프로세스\n",
    "1. 주어진 전체 데이터셋 분포 확인\n",
    "2. 전체 데이터 셋은 서로 다른 정규 분포 형태의 확률 분포 곡선으로 구성되어 있다고 가정 \n",
    "3. 전체 데이터셋을 구성하는 여러 개의 정규분포 곡선 추출\n",
    "   - 개별 데이터가 이 중 어떤 정규분포에 속하는지 결정\n",
    "   - 각각의 분포가 하나의 군집으로 형성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a94f5-6589-4697-a28f-863c763cd55b",
   "metadata": {},
   "source": [
    "GMM 모델 파라미터 추정 \n",
    "- 기댓값-최대화 (Expectation - Maximization, EM) 알고리즘 사용\n",
    "    - 각 군집의 중심, 모양, 크기, 방향 모델링 가능\n",
    "\n",
    "- EM 알고리즘의 단계\n",
    "    1. Initialization: 필요한 파라미터 θ (u, ∑, π)에 대해 초기값 선정\n",
    "    2. E(Expectation) step: 현재 θ를 통해 x가 특정 분포(군집)에 속할 사후확률 계산\n",
    "    3. M(Maximization) step: 계산된 사후확률을 통해 파라미터 θ (u, ∑, π)를 다시 추정\n",
    "    4. 수렴 조건이 만족될때까지 2~3 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5369e6-f976-457f-95a3-399cce2aab3c",
   "metadata": {},
   "source": [
    "1. GMM의 장점\n",
    "    - K-means보다 유연하게 다양한 데이터셋에 적용 가능\n",
    "      - 타원형 분포, 중첩된 군집 구조\n",
    " \n",
    "2. GMM의 단점\n",
    "   - 군집화를 위한 수행시간 오래 걸림\n",
    "   - 가정한 분포(가우시안 분포)에 맞지 않는 데이터일 경우, 계산 복잡도가 높아지고 성능 저하 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaba7e4-5b86-45b9-90cf-911c1db7f420",
   "metadata": {},
   "source": [
    "# 3. 군집화 평가 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5e799-677f-4f53-b428-e2a05c025004",
   "metadata": {},
   "source": [
    "1. 내부 평가(Internal Evaluation)\n",
    "   - 기본적인 군집 품질 평가를 위해 알고리즘 자체의 내부 평가 지표 사용\n",
    "     - k-means의 경우, SSE(왜곡) 사용\n",
    "    - 군집 내부 응집도와 군집 간 분리도의 정량적 측정\n",
    "      - 데이터 자체의 구조를 기반으로 군집 품질 판단\n",
    "    - 대표적인 지표: 실루엣 계수\n",
    "\n",
    "2. 외부 평가 (External Evaluation)\n",
    "    - 데이터셋에 정답 레이블이 존재하는 경우\n",
    "      - 실제 레이블과 군집화 결과 비교 가능 (비지도 학습이라고 하더라도)\n",
    "    - 군집화 정확성 객관적으로 평가\n",
    "    - 대표적인 지표: ARI(Adjusted Rand Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d144e-3db9-44b6-afb8-a17278780b72",
   "metadata": {},
   "source": [
    "# 1) 실루엣 계수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f74fe3-de1e-411e-aaef-e8cb0cb171da",
   "metadata": {},
   "source": [
    "- 클러스터 내 샘플들이 얼마나 조밀하게 모여 있는지를 측정하는 도구\n",
    "- 군집의 품질 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da48fa7-ccba-4bd2-9e54-9791ed88441f",
   "metadata": {},
   "source": [
    "실루엣 계수 계산 과정\n",
    "1. 샘플 x(i)와 동일한 클러스터 내 모든 다른 포인트 사이의 거리를 평균해, 클러스터 응집력 a(i) 계산\n",
    "2. 샘플 x(i)와 가장 가까운 클러스터의 모든 샘플 포인트 사이의 거리를 평균해, 클러스터 분리도 b(i) 계산\n",
    "3. 클러스터 응집력과 분리도 사이의 차이를 둘 중 큰 값으로 나누어, 실루엣 s(i) 계산\\\n",
    "   s(i) = b(i) - a(i) / max{b(i), a(i)}\n",
    "\n",
    "- 실루엣 계수는 -1~1 사이의 값 \n",
    "    - a(i) = b(i) -> 실루엣 계수는 0\n",
    "    - b(i) >> a(i) -> 실루엣 계수 1에 가까워짐 (이상적인 점수)\n",
    "        - 클러스터 분석의 목적: 분리도(b(i)) 최대화, 응집력(a(i)) 최대화 (군집 내 거리 최소화)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90da7e9-7bdc-4ee8-aaac-36fe8d1c1ecf",
   "metadata": {},
   "source": [
    "## 2) Dunn Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5da70e-22bb-4093-a8a9-de098ccad1ae",
   "metadata": {},
   "source": [
    "클러스터 간 최소 거리(분리도)와 클러스터 내 최대 거리 (응집도)의 비율을 계산해 군집화 품질 평가 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a815b4-5cb0-450f-be1e-040e2a6d554e",
   "metadata": {},
   "source": [
    "min d(C(i),C(j)) / max δ(C(k))\n",
    "- d(C(i),C(j)): 클러스터 C(i)와 C(j) 사이의 거리\n",
    "  - 일반적으로 클러스터 중심 사이 거리 / 두 클러스터 간 최소 거리\n",
    "- δ(C(k)): 클러스터 C(k) 내 최대 거리\n",
    "  - 해당 클러스터 안에서 가장 멀리 떨어진 두 점 간의 거리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1bc5a0-099c-48a4-b40a-69e54ff05d6c",
   "metadata": {},
   "source": [
    "- Dunn Index 해석\n",
    "  - 값이 클수록 좋은 군집화 결과\n",
    "  - 클러스터 분석의 목적: 군집 간 거리 최대화, 군집 내부 거리 최소화 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeba0192-96a1-4061-849c-d841c7f97c5f",
   "metadata": {},
   "source": [
    "- Dunn Index 한계\n",
    "  - 군집 수 증가할수록, 계산 비용 증가\n",
    "  - 이상치에 민감 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe223062-6c42-485b-991e-5d41dac42091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
