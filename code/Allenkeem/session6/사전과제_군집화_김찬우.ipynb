{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7차 교육세션 군집화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 군집화의 개념 및 목표에 대해 이해한다.\n",
    "* 데이터셋에 다양한 군집화 방법을 적용해보고 비교한다.\n",
    "* 군집화 결과를 평가하고 적절한 군집화 방법을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 군집화\n",
    "### (1) 머신러닝: 비지도학습\n",
    "\n",
    "* 비지도학습은 패턴이 알려지지 않았거나 계속해서 변화하는 문제를 해결할 때 유용하다. 구체적이고 좁은 범위에서 정의된 문제를 푸는 데는 지돡습보다 불리하지만, 더 열린 문제를 해결하고 지식을 일반화하는 데는 유리하다.\n",
    "\n",
    "* 데이터 자체에 내재된 구조를 학습하는 머신러닝 방법.\n",
    "\n",
    "* 정답 레이블이 없기에 에이전트가 해야 하는 작업이 명확히 정의되지 않는다. 따라서 **모델의 성능을 명확히 측정할 수 없다!**\n",
    "\n",
    "* 표현학습(or 피처 학습)을 수행함으로써 데이터셋의 고유 패턴을 식별한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 군집화란\n",
    "\n",
    "**군집화**: 데이터를 비슷한 특성을 가진 그룹(군집)으로 나누는 비지도 학습 기법.\n",
    "\n",
    "* 군집: 유사한 데이터들의 집합. 이때 서로 다른 군집에 속한 개체들은 비슷하지 않아야 한다.\n",
    "\n",
    "* **유사성**을 기반으로 개체를 함께 그룹화하는 방법\n",
    "* 하나의 관측치가 다른 관측치 또는 그룹의 데이터와 얼마나 유사한지 비교하는 작업\n",
    "\n",
    "*군집화에서는 종속 변수가 없기 때문에 특정한 목표 없이 패턴을 인식합니다. 즉, 군집화는 사전에 정해진 목표 없이 데이터셋에 존재하는 고유한 패턴을 발견하고 추출하는 도구*\n",
    "\n",
    "**목표**\n",
    "* 응집도 최대화: 같은 군집에 속하는 데이터끼리는 최대한 비슷하도록.\n",
    "* 분리도 최대화: 서로 다른 군집은 최대한 분리되도록\n",
    "\n",
    "→ 군집화를 통해 데이터 간의 유사성은 최대한 잘 유지하며, 서로 다른 그룹은 최대한 분리될 수 있도록 만드는 것이 목표!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 군집화 과정\n",
    "\n",
    "기본적으로는 다음과 같다.\n",
    "1. 피처 선택 또는 추출\n",
    "2. 군집화 알고리즘 선택\n",
    "3. 군집 유효성 검증\n",
    "4. 결과 해석\n",
    "\n",
    "고려해야하는 점\n",
    "* 변수 유형 이해: 변수 종류가 연속형 혹은 범주형, 또 변수 개수와 특징에 대한 이해가 필요\\\n",
    "→ 변수 유형에 따라 방법론 결정\n",
    "* 거리 (or 유사도) 정의와 측정. 어떤 방식으로 거리/유사도를 측정할 것인가.\\\n",
    "→ 회귀에서는 변수 자체가 중요했지만, 군집화에서는 거리를 어떻게 정의하고 측정할 것인지가 더 중요하다.\\\n",
    "∵ 거리를 측정하는 방법이 변수의 특성과 관계있기에\n",
    "* 차원 축소: 유사한 변수들을 묶어서 차원을 축소\n",
    "\n",
    "*군집화는 많은 경우에 여러 번의 반복적인 시도를 요구한다. 또한 피처와 군집화 스키마 선택 시 명확한 기준이 없기 때문에, 적합한 평가 기준을 고르는 것조차 하나의 문제가 될 수 있다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 군집화 알고리즘\n",
    "\n",
    "### (1) 계층적 군집화\n",
    "**계층적 군집화**: 데이터 간의 유사성을 기반으로 트리 구조를 형성하여, 상향식 또는 하향식 방식으로 군집을 형성해 나가는 방법\n",
    "\n",
    "계층적 군집화 알고리즘은 데이터셋의 관측치를 사용해 덴드로그램을 만든다.\\\n",
    "***덴드로그램**: 가까운 두 개체 또는 군집을 점차적으로 병합해가는 과정을 시각적으로 표현한 트리 구조*\n",
    "\n",
    "\n",
    "* 계층적 군집화에서는 군집의 개수를 사전에 설정하지 않고, 클러스터링이 종료된 후에 원하는 군집의 개수를 선택한다.\n",
    "    * 상향식: 응집형 계층적 군집화 (or 병합 클러스터링): 먼저 각 샘플이 독립적인 클러스터가 된 후, 하나의 클러스터만 남을 때까지 가장 가까운 클러스터를 병합한다.\n",
    "    * 하향식: 분리형 계층적 군집화 (or 분할 클러스터링): 전체 샘플을 포함하는 하나의 클러스터에서 시작하여, 유사성이 낮은 데이터들을 더 작은 클러스터로 나눈다.\n",
    "\n",
    "위 두 방법은 모두 데이터를 proximity matrix (거리 행렬/유사도 행렬)에 기반한 계층적 구조로 조직화한다. 계층적 군집화의 결과는 일반적으로 이진 트리 혹은 덴드로그램으로 표현된다.\n",
    "\n",
    "**이번 세션은 응집형 (상향식)을 집중적으로**\n",
    "\n",
    "응집형 계층적 군집화는 수 회의 병합 연산을 통해 클러스터들을 합쳐 더 큰 클러스터로 만든다.\\\n",
    "계속해서 병합 연산을 한다면 최종적으로는 모든 데이터포인트들이 같은 그룹에 속하게 된다.\\\n",
    "이 때, 가까운 군집 간 병합이 일어나기 때문에 **군집 간의 거리를 어떻게 정의**하느냐에 따라 알고리즘이 달라지게 된다.\n",
    "\n",
    "가장 기본적인 거리 정의 알고리즘은 단일 연결과 완전 연결이다.\n",
    "1. **단일 연결**: 클러스터 쌍에서 가장 비슷한 샘플 간 거리를 계산. 이후 이 거리가 가장 작은 두 클러스터를 병합\n",
    "2. **완전 연결**: 가장 비슷한 샘플이 아닌 가장 비슷하지 않은 샘플을 비교해 병합을 수행\n",
    "\n",
    "이외에도 다른 알고리즘에는:\\\n",
    "* 평균 연결: 모든 항목에 대한 거리 평균을 구한다.\n",
    "    * 계산량이 불필요하게 많아질 수 있다는 단점이 있다.\n",
    "* 중심 연결 (centroid method): 두 클러스터의 중심 간의 거리를 측정. 두 군집이 결합될 때 새로운 군집의 평균은 가중평균을 통해 구한다.\n",
    "    * 이 역시 시간이 오래 걸린다는 단점이 있다.\n",
    "* 중앙 연결 (Median): 군집간의 거리를 군집 내 모든 샘플의 중앙값으로 정의. 데이터의 중앙값에 기반해 군집 간 거리를 정의하므로 극단값에 영향을 덜 받는다.\n",
    "    * 기하학적 구조 파악 어렵다\n",
    "* Ward's Procedure (Ward 연결법)\n",
    "    * 군집의 병합 후 군집 내 SSE(오차제곱합)의 증가분 최소 인것을 선택. 보통 두 군집이 합해지면 병합된 군집의 SSE는 병합 이전 각 군집의 SSE의 합보다 커지게 되는데 그 증가량이 가장 작아지는 방향으로 군집을 형성해 나간다.\n",
    "\n",
    "---\n",
    "**완전 연결 방식을 사용한 응집형 계층 군집화 과정**\n",
    "1. 모든 샘플의 거리 행렬을 계산\n",
    "2. 모든 데이터 포인트를 단일 클러스터로 표현\n",
    "3. 가장 비슷하지 않은 샘플 사이 거리에 기초하여 가장 가까운 두 클러스터를 합침\n",
    "4. 유사도 행렬을 업데이트\n",
    "5. 하나의 클러스터가 남을 때까지 2~4번 반복 \n",
    "---\n",
    "\n",
    "*계층적 군집화는 계산량이 많기에 대규모 데이터보다는 소규모 데이터에 대해 군집 수를 탐색하거나 클러스터링의 전반적인 구조를 시각적으로 분석할 때 유용하게 활용할 수 있다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) k-means\n",
    "\n",
    "**k-means**: 데이터를 정해진 개수(K)의 그룹으로 나누되, 각 그룹의 중심점(centroid)과의 거리가 가장 가까운 데이터끼리 묶는 군집화 알고리즘\n",
    "\n",
    "* 군집화에서 가장 일반적으로 사용되는 알고리즘\n",
    "* **연속적인** 특성을 갖는 데이터 샘플들 중에서 군집 중심점을 선택해 해당 중심에 가장 가까운 데이터들을 묶어나가는 군집화 기법\n",
    "---\n",
    "*이는 프로토타입 기반 군집화의 일종이다.*\\\n",
    "*프로토타입 기반 군집화: 각 클러스터가 하나의 프로토타입으로 표현된다는 것*\\\n",
    "*k-means의 경우 centroid가 프로토타입이 된다.*\n",
    "\n",
    "---\n",
    "\n",
    "k-means 알고리즘의 주요 단계:\n",
    "\n",
    "1. 데이터 표본들 중에서 랜덤하게 k개의 중심점을 초기 클러스터 중심으로 선택\n",
    "2. 각 표본들을 가장 가까운 중심점 μ에 할당 *k-means에서는 대게 유클리디안 거리의 제곱을 사용한다*\n",
    "3. 각 클러스터에 할당된 표본들의 데이터 평균을 계산하여 중심점을 이동\n",
    "4. 클러스터 할당이 변하지 않거나 사용자가 지정한 허용 오차 또는 최대 반복 횟수(`max_iter`)에 도달할 때까지 2~3을 반복\n",
    "\n",
    "k-means 알고리즘은 원형 클러스터를 구분하는 데 뛰어나지만, 사전에 사용자가 클러스터 개수를 직접 지정해줘야 한다는 단점이 있다.\\\n",
    "→적절하지 않은 k 선택시 군집 성능이 약화된다.\\\n",
    "∴ 적절한 k 값을 찾기 위해 엘보우 방법과 실루엣 계수 등을 활용할 수 있다.\n",
    "\n",
    "k-means 알고리즘을 유클리디안 거리 지표를 사용해 실제 데이터에 적용할 때 특성이 같은 스케일로 측정되었는지 확인해야 한다. 필요하다면 이를 위해 Z-Score 표준화 혹은 Min-Max Scaler로 변환해야 한다.\n",
    "\n",
    "---\n",
    "```python\n",
    "# 사이킷런의 KMeans 클래스\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, # 여기서 클러스터 개수(k)를 지정!\n",
    "                init='random', \n",
    "                ...,\n",
    "                random_state=42)\n",
    "```\n",
    "---\n",
    "\n",
    "k-means의 장단점:\n",
    "* k-means의 장점\n",
    "    * 직관적이고 구현이 쉽다.\n",
    "    * 대용량 데이터에도 적용이 가능\n",
    "* k-means의 단점\n",
    "    * 초기 centroid 값에 민감\n",
    "    * 군집 수(k)의 결정이 어렵다\n",
    "    * 아웃라이어에 민감\n",
    "    * 기하학적인 모양의 군집은 파악하기 어렵 (↔ 원형 클러스터는 구분 쉽다고 위에서 그랬죠?)\n",
    "        * 이 문제를 해소하기 위해 k-Median, K-Medoids, Mean Shift 알고리즘을 고려해볼 수 있다.\n",
    "    * **빈 클러스터 문제**\n",
    "        * 클러스터가 비어있을 수 있다. *다행히 사이킷런의 k-means에서는 해결책 마련해둠.*\n",
    "\n",
    "k-means에서는 초기 중심점에 따라 최종 군집화의 품질이 달라지거나 알고리즘 성능이 하락하는 단점 존재\n",
    "이를 해결하기 위해서는 k-means 알고리즘을 여러 번 실행하여 SSE가 제일 잘 나오는 모델을 선택할 수도 있지만, **k-means++ 알고리즘**을 통해 초기 중심점들을 서로 멀리 떨어진 곳에 위치시킬 수도 있다.\n",
    "\n",
    "k-means++ 알고리즘의 초기화 과정:\n",
    "1. 선택한 k개의 중심점을 저장할 빈 집합 M을 초기화\n",
    "2. 입력 샘플에서 첫 번째 중심점 μ을 랜덤하게 선택하고 M에 할당\n",
    "3. M에 없는 각 샘플 x에 대해 M에 있는 중심점까지의 최소 제곱 거리 d를 찾는다.\n",
    "4. 가중치가 적용된 확률 분포를 사용하여 다음 중심점 μ을 랜덤하게 선택\n",
    "5. k개의 중심점을 선택할 때까지 3,4번을 반복\n",
    "6. 기본 k-means 알고리즘을 수행\n",
    "\n",
    "사이킷런에서 k-means++를 사용하려면 `KMeans`의 `init` 파라미터를 k-means++로 지정하면 된다. *(사실 기본값이다.)*\n",
    "\n",
    ">엘보우 방법(elbow method)\n",
    "\n",
    "클래스 내 SSE를 바탕으로 **엘보우 방법**이라고 하는 그래프를 활용하여 최적 클러스터 개수 k를 추정할 수 있다.\n",
    "\n",
    "* 사이킷런 `KMeans` 클래스의 `inertia_` 속성값을 확인하면 SSE를 알 수 있다.\n",
    "\n",
    "```\n",
    "print('왜곡: %.2f' % km.inertia_)\n",
    "```\n",
    "\n",
    "엘보우 방법의 아이디어는 왜곡이 빠르게 감소하는 지점의 k값을 찾는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) DBSCAN\n",
    "**DBSCAN**: 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘\n",
    "\n",
    "DBSCAN 알고리즘의 주요 단계:\\\n",
    "1. 다음 조건에 따라 각 샘플을 핵심 샘플, 경계 샘플, 잠음 샘플 중 하나에 할당.\n",
    "---\n",
    "* 어떤 샘플의 특정 반경 안에 있는 이웃 샘플이 지정된 개수(MinPts) 이상이면 **핵심 샘플**\n",
    "* 특정 반경 이내에 MinPts보다 이웃이 적지만 다른 핵심 샘플의 반경 안에 있으면 **경계 샘플**\n",
    "* 위 두 샘플에 해당하지 않는 다른 모든 샘플은 **잡음 샘플**\n",
    "---\n",
    "2. 개별 핵심 샘플 또는 (특정 반경 이내에 있는 핵심 샘플을 연결한) 핵심 샘플의 그룹을 클러스터로 만든다.\n",
    "3. 경계 샘플을 해당 핵심 샘플의 클러스터에 할당\n",
    "\n",
    "**하이퍼 파라미터**\n",
    "* `eps`: 데이터 포인트 사이의 최대 거리\n",
    "* `min_samples`: 포인트가 군집이 되기 위해 eps 거리 내 포인트가 얼마나 많아야 하는지\n",
    "\n",
    "→ 어느 데이터 포인트를 기준으로 반경 eps 내 기준이 되는 데이터 포인트를 포함하여 데이터 포인터의 개수가 min_samples 이상이면 하나의 군집으로 인식\n",
    "\n",
    "> DBSCAN의 장단점\n",
    "* 장점\n",
    "    * k-means처럼 클러스터 모양을 원형으로 가정하지 않고, 밀도에 따라 클러스터를 할당 → 임의의 기하학적 분포를 갖는 데이터도 잘 처리한다.\n",
    "    * 이상치에 민감하지 않다\n",
    "    * k-means나 계층 군집과 달리 모든 샘플을 클러스터에 할당하지 않는다.\n",
    "\n",
    "* 단점\n",
    "    * 'epsilon'과 'min_samples' 설정에 많은 영향을 받음\n",
    "    * 'epsilon'을 조절하기 쉽지 않으며, 적절한 조절을 위해서 데이터셋에 대한 도메인 지식이 필요한 경우가 많다.\n",
    "    * 연산량이 많아 속도가 느리다.\n",
    "    * 다른 밀도 분포를 가진 데이터의 군집 분석을 잘 하지 못한다.\n",
    "    * '차원의 저주'를 벗어나지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Gaussian Mixture Model (GMM)\n",
    "\n",
    "**Gaussian Mixture Model**: 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 군집화 방법\n",
    "\n",
    "* 모델 기반 군집화 방법 중 하나\n",
    "    * 모델 기반 군집화: 데이터를 생성하는 통계적 모델을 가정하고, 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화를 수행하는 접근 방식\n",
    "* 각 군집을 확률분포로 간주\n",
    "* 전체 데이터 분포를 이질적인 여러 개의 확률 분포의 혼합으로 보고 모델링한다 \\\n",
    "→ 거리 기반 군집화와 차이. 데이터의 생성 메커니즘을 모델링할 수 있다는 특징이 있다.\n",
    "\n",
    "> GMM의 가정\n",
    "* 관측된 데이터 포인트들은 특정 가우시안 확률 분포에 의해 생성되었다고 가정.\n",
    "* 군집화를 적용하고자 하는 전체 데이터셋에는 여러 개의 가우시안 분포가 섞여있고, 개별 데이터는 우도(가능도)에 따라 K개의 가우시안 분포 중 한 가지에 속한다.\\\n",
    "*우도: 고정된 관측값이 어떠한 확률분포에서 어느정도의 확률로 나타나는지에 대한 확률*\n",
    "\n",
    "> GMM의 진행 과정\n",
    "1. 주어진 전체 데이터셋의 분포 확인\n",
    "2. 전체 데이터 셋은 서로 다른 정규 분포 형태의 확률 분포 곡선으로 구성되어 있다고 가정\n",
    "3. 전체 데이터셋을 구성하는 여러 개의 정규분포 곡선 추출, 개별 데이터가 이 중 어떤 정규분포에 속하는지 결정. 이때 각각의 분포가 하나의 군집이 된다.\n",
    "\n",
    "> GMM의 장단점\n",
    "* 장점\n",
    "    * K-means보다 유연하게 다양한 데이터 세트에 잘 적용될 수 있다: k-means가 잘 작동하지 않는 타원형 분포나, 중첩된 군집 구조에서도 좋은 성능을 낼 수 있다.\n",
    "* 단점\n",
    "    * 군집화를 위한 수행시간이 오래 걸림\n",
    "    * 가정한 분포(가우시안 분포)에 맞지 않는 데이터일 경우 계산 복잡도가 높아지고 성능이 저하될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 군집화의 평가 방법\n",
    "\n",
    "데이터셋에 label이 없기에 이전에 이용한 지도 학습 알고리즘들의 성능 평가를 위해 사용한 기법들을 비지도 학습에는 적용할 수 없다.\n",
    "    \n",
    "∴군집화에서는 기본적으로 군집 품질을 평가하기 위해 알고리즘 자체의 **내부 평가 지표**를 사용해야 한다. *예컨대 k-means에서는 클래스 내 SSE를 사용하였다.*\n",
    "\n",
    "내부 평가는 군집 내부의 응집도와 군집 간의 분리도를 정량적으로 측정하여 데이터 자체의 구조를 기반으로 군집 품질을 판단한다.\n",
    "\n",
    "한편 데이터셋에 정답 레이블이 존재하는 경우에도 비지도 학습 방법을 적용해볼 수 있다. 이 경우 **외부 평가 지표**를 활용해 군집화 결과를 평가할 수 있다. 외부 평가는 실제 레이블(정답)이 존재하는 상황에서, 군집화 결과와 실제 클래스 간의 일치도를 비교함으로써 군집화의 정확성을 객관적으로 평가함.\n",
    "\n",
    "1. 내부 평가 (Internal Evaluation)\n",
    "* 정답 라벨이 없는 경우: 군집 내부의 응집도와 군집 간의 분리도를 기준으로 군집 품질을 평가한다.\n",
    "* 대표적인 지표\n",
    "    * 실루엣 계수: 한 점이 자기 클러스터 내에서 얼마나 가깝고, 다른 클러스터와는 얼마나 멀리 떨어져 있는지 측정\n",
    "2. 외부 평가 (External Evaluation)\n",
    "* 정답 라벨이 있는 경우: 실제 라벨과 군집화 결과를 비교할 수 있다.\n",
    "* 대표적인 지표\n",
    "    * ARI (Adjusted Rand Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 실루엣 계수\n",
    "**실루엣 계수**: 클러스터 내 샘플들이 얼마나 조밀하게 모여 있는지를 측정하는 도구\n",
    "\n",
    "> 실루엣 계수 계산 과정\n",
    "1. 샘플 x와 동일한 클러스터 내 모든 다른 포인트 사이의 거리를 평균하여 **클러스터 응집력 a**를 계산한다.\n",
    "2. 샘플 x와 가장 가까운 클러스터의 모든 샘플 간 평균 거리로 최근접 클러스터의 **클러스터 분리도 b**를 계산한다.\n",
    "3. 클러스터 응집력과 분리도 사이의 차이를 둘 중 큰 값으로 나누어 실루엣 s를 계산한다\n",
    "\n",
    "실루엣 계수는 -1 ~ 1 사이 값을 가진다.\n",
    "* 클러스터 응집력과 분리도가 같다면 실루엣 계수는 0이 된다.\n",
    "* b >> a이면 이상적인 실루엣 계수인 1에 가까워진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Dunn Index\n",
    "\n",
    "**Dunn Index**: 클러스터 간 최소 거리(분리도)와 클러스터 내 최대 거리(응집도)의 비율을 계산해 클러스터링의 품질을 평가하는 지표\n",
    "\n",
    "**Dunn Index 값이 클수록 좋은 군집화 결과를 의미한다.**\n",
    "\n",
    "> Dunn Index의 한계\n",
    "* 군집 수가 많아질수록 계산 비용이 증가\n",
    "* 이상치에 민감할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
