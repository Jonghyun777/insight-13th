{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#FFE6E6; font-size:40px; font-weight:bold; color:#000000;\">\n",
    "# 1. 군집화(Clustering)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 머신러닝: 비지도 학습\n",
    "### 🎯 비지도 학습\n",
    "* 정답 레이블이 없음 ➡️ 에이전트가 해야 하는 작업이 명확히 정의되지 않음 ➡️ **모델의 성능**을 명확히 측정할 수 없음\n",
    "* 표현 학습(representation learning, 또는 feature learning)을 수행함으로써 데이터셋의 고유한 패턴을 식별할 수 있음\n",
    "  \n",
    "## 1-2. 군집화(clustering)란?\n",
    "### 🎯 군집화(clustering)\n",
    "* 데이터를 **비슷한 특성을 가진 그룹(군집)** 으로 나누는 비지도학습 기법.\n",
    "* **유사성**을 기반으로 개체들을 함께 그룹화하는 방법.\n",
    "* *하나의 관측치*가 *다른 관측치 또는 그룹의 데이터*와 얼마나 유사한지 비교하는 작업.\n",
    "* 각 그룹이 어떻게 구성되어 있는지 파악하면서, 전체 데이터 구조를 이해할 수 있음.\n",
    "  \n",
    "<img src=\"./img/img1.webp\" width=300>\n",
    "\n",
    "> **군집(cluster)**\n",
    "> * 유사한 데이터들의 집합. \n",
    "> * 이때 서로 다른 군집에 속한 개체들은 비슷하지 않아야 함.\n",
    "### 📍 군집화의 목표\n",
    "* **응집도(cohesion) 최대화**: 같은 군집에 속하는 데이터끼리는 최대한 비슷하도록 함.\n",
    "* **분리도(separation) 최대화**: 서로 다른 군집은 최대한 분리되도록 함. <br>\n",
    "➡️ 군집화를 통해 **데이터 간의 유사성을 최대한 잘 유지**하면서, **서로 다른 그룹은 구분**될 수 있도록 만드는 것이 목표!\n",
    "\n",
    "## 1-3. 군집화 과정\n",
    "### 📍 군집화의 기본 과정\n",
    "* 1️⃣ 피처 선택 또는 추출\n",
    "* 2️⃣ 군집화 알고리즘 선택\n",
    "* 3️⃣ 군집 유효성 검증 \n",
    "* 4️⃣ 결과 해석\n",
    "\n",
    "<img src = \"./img/img2.webp\" width=400>\n",
    "\n",
    "### 📌 군집화의 주요 고려 사항\n",
    "* **1️⃣ 변수 유형 이해**\n",
    "  * 변수 종류(연속형/명목형), 변수 개수, 변수 특징에 대한 이해가 필요함\n",
    "* **2️⃣ 거리(유사도) 정의와 측정**\n",
    "  * 군집화는 **데이터 간의 유사도**를 기반으로 이루어짐\n",
    "  * *어떤 방식*으로 **거리(distance)** 또는 **유사도(similarity)** 를 측정하는지가 중요함\n",
    "  * cf) `회귀 분석`: 변수 자체가 중요했음 ↔︎ `군집 분석`: 거리를 어떻게 정의하고 측정할지가 더 중요함 (거리 측정 방법이 변수의 특성과 관계가 있기 때문)\n",
    "* **3️⃣ 차원 축소**\n",
    "  * 유사한 변수들을 묶어서 차원을 축소\n",
    "  * ex) 연봉, 나이, 연차, 직군 등 다양한 변수가 있는 경우\n",
    "    * 나이와 연차는 비슷한 변수로 묶을 수 있음\n",
    "    * 나이를 제외하고 연차 하나만 사용함으로써 차원 축소 가능\n",
    "  \n",
    "### 📍 군집화의 특징\n",
    "* 군집화는 많은 경우에서 **여러 번의 반복적인 시도**를 요구함\n",
    "* feature 선택과 군집화 스키마 선택 시 명확한 기준이 없음\n",
    "  * ➡️ **적합한 평가 기준을 고르는 것** 또한 하나의 문제가 될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#FFE6E6; font-size:40px; font-weight:bold; color:#000000;\">\n",
    "# 2. 군집화 알고리즘\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. 계층적 군집화(Hierarchical Clustering)\n",
    "### 🎯 계층적 군집화\n",
    "* 데이터 간의 **유사성**을 기반으로 **트리 구조(dendrogram)** 를 형성하는 방법\n",
    "* **계층적 군집화 알고리즘**은 데이터셋의 관측치를 사용해 **덴드로그램**을 만듦\n",
    "  * **덴드로그램(Dendrogram)** 이란? <br> <img src=\"./img/img3.webp\" width=400> <br>\n",
    "    * 가까운 두 개체 또는 군집을 **점차적으로 병합**해나가는 과정을 시각적으로 표현한 **트리 구조**\n",
    "    * **덴드로그램 해석**: Dendrogram의 `수직축(높이)`= `두 군집이 병합될 때의 군집 간 거리(유사성)`\n",
    "      * ex) 위의 예시에서 E, F는 서로 연결되는 링크의 높이가 가장 짧음(E와 F가 가장 가까움)\n",
    "  * 계층적 군집화: 군집의 개수를 사전에 설정하지 않고, 클러스터링이 종료된 후 원하는 군집의 개수를 선택함. \n",
    "* **상향식 또는 하향식 방식**으로 군집을 형성해나가는 방법 <br> <img src=\"./img/img4.webp\" width=400>\n",
    "\n",
    "  * 1️⃣ **상향식: 응집형 계층적 군집화(agglomerative hierarchical clustering, 병합 클러스터링)**\n",
    "    * 먼저 각 샘플이 독립적인 클러스터가 됨\n",
    "    * 이후 하나의 클러스터만 남을때까지 가장 가까운 클러스터를 병합함\n",
    "  * 2️⃣ **하향식: 분리형 계층적 군집화(divisive hierarchical clustering, 분할 클러스터링)**\n",
    "    * 전체 샘플을 포함하는 하나의 클러스터에서 시작함\n",
    "    * 유사성이 낮은 데이터들을 더 작은 클러스터로 나눔\n",
    "\n",
    "* 계층적 군집화의 두 방법은 모두 데이터를 **proximity matrix(거리 행렬 또는 유사도 행렬)** 에 기반한 **계층적 구조로 조직화**함.\n",
    "* 계층적 군집화의 결과는 일반적으로 `이진 트리` 또는 `dendrogram`으로 표현됨.\n",
    "  * 두 계층적 군집화의 dendrogram 예시 <br> <img src=\"./img/img5.webp\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 응집형 계층적 군집화\n",
    "* 각각 **1개의 데이터포인트만을 포함**하는 **N개의 클러스터**에서 시작함\n",
    "* 이후 수 회의 **병합 연산**을 통해 이 클러스터들을 합쳐 **더 큰 클러스터로** 만듦\n",
    "  * 계속해서 병합 연산을 한다면, 최종적으로는 *모든 데이터포인트들이 같은 그룹에 속하게* 될 것임\n",
    "  * 이때, 가까운 군집 간 병합이 일어나기 때문에, **군집 간의 거리를 어떻게 정의하느냐**에 따라 알고리즘이 달라짐.\n",
    "\n",
    "### 📍 6가지 거리 정의 방법\n",
    "#### 1️⃣ Single linkage (단일 연결법/최단 연결법)\n",
    "* *서로 다른 군집에서 가장 가까운 두 점 사이의 거리*를 *군집 간의 거리*로 측정하는 방법\n",
    "* **고립된 군집**을 찾는 데 중점을 두고 있음\n",
    "* 이상치에 취약함 <br> <img src=\"./img/img6.webp\" width=400>\n",
    "* **방법**\n",
    "  * 클러스터 쌍에서 가장 비슷한 샘플 간의 거리를 계산\n",
    "  * 이후 이 거리가 가장 작은 두 클러스터를 병합\n",
    "\n",
    "#### 2️⃣ Complete linkage (완전 연결법/최장 연결법)\n",
    "* *서로 다른 군집에서 가장 비슷하지 않은 샘플*을 비교해 병합을 수행함\n",
    "* 군집에서 하나씩 관측값을 뽑았을 때 *나타날 수 있는 거리의 최댓값*을 측정하여 *가장 유사성이 큰 군집으로 병합*해 나가는 방법\n",
    "* **군집들의 내부 응집성**에 중점을 둔 방법\n",
    "* 이상치에 민감함 <br> <img src=\"./img/img7.webp\" width=400>\n",
    "\n",
    "#### 3️⃣ Average linkage (평균 연결법)\n",
    "* **모든 항목에 대한 거리 평균**을 구함\n",
    "* 단점: 계산량이 불필요하게 많아질 수 있음\n",
    "* 장점: 최단 연결법, 최장 연결법보다 이상치에 덜 민감함 <br> <img src=\"./img/img8.webp\" width=400>\n",
    "  \n",
    "#### 4️⃣ Centroid Method (중심 연결법)\n",
    "* 두 군집의 **중심 간의 거리를 측정**함\n",
    "* 두 군집이 결합될 때 *새로운 군집의 평균*은 *가중평균*을 통해 구함\n",
    "* 단점: 군집들의 중심을 계산해야 하기 때문에 시간이 오래 걸림 <br> <img src=\"./img/img9.webp\" width=400>\n",
    "  \n",
    "#### 5️⃣ Median (중앙 연결법)\n",
    "* 군집 간의 거리를 **군집 내 모든 샘플의 중앙값**으로 정의함\n",
    "* 장점: 데이터의 중앙값에 기반해 군집 간의 거리를 정의하므로 *극단값의 영향을 덜 받음*\n",
    "* 단점: 기하학적 구조를 파악하기는 어려움\n",
    "\n",
    "#### 6️⃣ Ward's Procedure(Ward 연결법)\n",
    "* 군집의 **병합 후** 군집 내 **SSE(오차제곱합)의 증가분 최소**인 것을 선택함\n",
    "* 보통 두 군집이 합해지면 `병합된 군집의 SSE`는 `이전 각 군집의 SSE의 합`보다 커지게 됨. 이때 **그 증가량이 가장 작아지는** 방향으로 군집을 형성해 나가는 방법. <br> <img src=\"./img/img10.webp\" width=400>\n",
    "\n",
    "\n",
    "> ##### 이 중 2️⃣ 완전 연결 방식을 사용한 응집형 계층 군집화의 과정\n",
    "> 1. 모든 샘플의 거리 행렬을 계산한다.\n",
    "> 2. 모든 데이터 포인트를 단일 클러스터로 표현한다.\n",
    "> 3. 가장 비슷하지 않은 샘플 사이의 거리에 기초하여, 가장 가까운 두 클러스터를 합친다.\n",
    "> 4. 유사도 행렬을 업데이트 한다.\n",
    "> 5. 하나의 클러스터가 남을 때까지 2~4를 반복한다.\n",
    "\n",
    "### 📍 계층적 군집화 결과 시각화하기\n",
    "* **덴드로그램**을 통해 계층적 군집화가 수행되는 동안 만들어지는 **클러스터들을 요약해서 확인**할 수 있음. <br> <img src=\"./img/img11.webp\" width=400>\n",
    "  * 위 그림의 경우 `ID_0`과 `ID_4`, `ID_1`과 `ID_2`가 유클리디안 거리 측정 기반으로 했을 때 가장 가까운 샘플임을 알 수 있음.\n",
    "* 계층적 군집화는 *계산량이 많기 때문*에 대규모 데이터에 적용하기 보다는 **소규모 데이터**에 대해 **군집 수를 탐색**하거나 **클러스터링의 전반적인 구조를 시각적으로 분석**할 때 유용하게 활용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. k-means\n",
    "### 🎯 k-means\n",
    "* 연속적인 특성을 갖는 데이터를 정해진 개수(k)의 그룹으로 나누되, **각 그룹의 군집 중심점(centroid)과의 거리가 가장 가까운 데이터**끼리 묶는 군집화 알고리즘.\n",
    "* 군집화에서 가장 일반적으로 사용되는 알고리즘임. <br> <img src=\"./img/img12.gif\" width=400>\n",
    "\n",
    "### 📍 k-means 알고리즘의 주요 단계\n",
    "<br> <img src=\"./img/img13.webp\" width=600>\n",
    "\n",
    "1. 데이터 표본들 중에서 **랜덤하게 K개의 중심점(centroid)** 을 **초기 클러스터 중심**으로 선택한다.\n",
    "2. 각 표본들을 가장 가까운 중심점 $\\mu^{(j)}, j\\in {1, ..., k}$에 할당한다.\n",
    "   * 두 샘플의 거리를 측정하는 방법: k-means에서는 보통 **유클리디안 거리의 제곱(squared Euclidean distance)** 을 사용\n",
    "3. 각 클러스터에 할당된 표본들의 데이터 평균을 계산하여 **중심점(centroid)을 이동**시킨다.\n",
    "4. 클러스터 할당이 변하지 않거나, 사용자가 지정한 허용 오차 또는 최대 반복 횟수(`max_iter`)에 도달할 때까지 2~3을 반복한다.\n",
    "\n",
    "#### [Code] 사이킷런의 `KMeans` 클래스\n",
    "> ```python\n",
    "> from sklearn.cluster import KMeans\n",
    "> kmeans = KMeans(n_clusters=3, # 여기서 클러스터 개수(k)를 지정!\n",
    ">    \t        init='random', \n",
    ">                 ...,\n",
    ">                 random_state=42)\n",
    "> ```\n",
    "\n",
    "### 📍 k-means의 장단점\n",
    "#### 👍 장점\n",
    "* 직관적이고 구현이 쉬움\n",
    "* 대용량 데이터에도 적용 가능\n",
    "#### 👎 단점\n",
    "* **초기 centroid 값에 민감함**\n",
    "  * 초기 centroid를 설정하는 많은 방법이 존재함\n",
    "  * 이에 따라 형성되는 최종적인 군집이 달라질 수 있음\n",
    "* **군집 수(K) 결정이 어려움**\n",
    "  * 사용자가 직접 K값을 지정해야 함\n",
    "  * 이 k값이 결과의 질에 큰 영향을 미침\n",
    "* **극단값(outlier)에 민감함**\n",
    "  * 평균 중심으로 군집을 구성하기에, **outlier 하나가 중심을 왜곡**시킬 수 있음\n",
    "* **기하학적 모양의 군집을 파악하기 어려움**\n",
    "* **빈 클러스터 문제**\n",
    "  * 비어 있는 클러스터가 존재할 수도 있음\n",
    "  * 사이킷런의 K-means에서는 크게 신경쓰지 않아도 됨 (`k-means++`)\n",
    "### 📍 k-means++ 알고리즘\n",
    "* k-means의 문제점\n",
    "  * 초기 중심점을 랜덤하게 할당 ➡️ 최종 군집화의 품질이 달라지거나 알고리즘 성능 하락\n",
    "* 해결 방법 1\n",
    "  * k-means 알고리즘을 여러 번 실행\n",
    "  * SSE 입장에서 가장 성능이 좋은 모델을 선택\n",
    "* 해결 방법 2\n",
    "  * k-means++ 알고리즘 사용\n",
    "  * 초기 중심점들을 서로 멀리 떨어진 곳에 위치시킴\n",
    "  * 기본적인 K-means보다 일관되고 좋은 결과를 도출할 수 있음\n",
    "\n",
    "#### [k-means++ 알고리즘의 초기화 과정]\n",
    "1. 선택한 $k$개의 중심점을 저장할 빈 집합 $M$을 초기화한다.\n",
    "2. 입력 샘플에서 첫 번째 중심점 $\\mu^{(i)}$을 랜덤하게 선택하고 $M$에 할당한다.\n",
    "3. $M$에 없는 각 샘플 $\\mathbf{x}^{(i)}$에 대해 $M$에 있는 중심점까지의 최소 제곱 거리 $d(\\mathbf{x}^{(i)}, M)^2$을 찾는다.\n",
    "4. $\\frac{d(\\mathbf{\\mu}^{(p)}, \\mathbf{M})^2}{\\sum_i d(\\mathbf{x}^{(i)}, \\mathbf{M})^2}$과 같은 가중치가 적용된 확률 분포를 사용해 다음 중심점 $\\mu^{(p)}$을 랜덤하게 선택한다.\n",
    "5. $k$개의 중심점을 선택할 때까지 3~4를 반복한다.\n",
    "6. 기본 K-means 알고리즘을 수행한다.\n",
    "\n",
    "* 사이킷런에서 k-means++ 사용하려면?\n",
    "  * `KMeans`의 `init` 파라미터를 `k-means++`로 지정하면 됨 (default임)\n",
    "\n",
    "### 📍 엘보우 방법(elbow method)\n",
    "* 최적 클러스터 개수 K를 추정하는 방법\n",
    "* k가 증가하면 왜곡이 줄어들 것임 (샘플이 할당된 중심점에 더 가까워지기 때문) <br> <img src=\"./img/img14.webp\" width=400>\n",
    "* 왜곡이 빠르게 감소하는 지점의 K값을 찾는 것\n",
    "* k값을 바꾸어 가며 k-means 클러스터링을 진행\n",
    "* 각 경우의 왜곡 값을 그래프로 그림\n",
    "* 위 그림의 경우, k=3이 가장 적절한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. DBSCAN (밀도 기반 클러스터링)\n",
    "### 🎯 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "* 밀도가 높은 지역의 데이터를 하나의 군집으로 묶음\n",
    "* 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘\n",
    "### 📍 DBSCAN 알고리즘의 주요 단계\n",
    "1. 각 샘플을 다음 조건에 따라 **핵심 샘플, 경계 샘플, 잡음 샘플** 중 하나에 할당한다. <br> <img src=\"./img/img15.webp\" width=400>\n",
    "   1. **핵심 샘플(core point)**: 어떤 샘플의 특정 반경 $\\epsilon$ 안에 있는 이웃 샘플이 *지정된 개수(MinPts) 이상*이면 핵심 샘플이 됨.\n",
    "   2. **경계 샘플(border point)**: $\\epsilon$ 이내에 MinPts보다 이웃이 적지만, 다른 핵심 샘플의 반경 $\\epsilon$ 안에 있으면 경계 샘플이 됨.\n",
    "   3. **잡음 샘플(noise point)**: 위 두 샘플에 해당하지 않는 다른 모든 샘플\n",
    "2. 개별 핵심 샘플 또는 핵심 샘플의 그룹을 클러스터로 만든다.\n",
    "3. 경계 샘플을 해당 핵심 샘플의 클러스터에 할당한다.\n",
    "### 📍 DBSCAN의 장단점\n",
    "#### 👍 장점\n",
    "* 임의의 **기하학적 분포**를 갖는 데이터도 **잘 처리**할 수 있음\n",
    "  * k-means처럼 클러스터 모양을 원형으로 가정하지 않고, **밀도에 따라 클러스터를 할당**하기 때문\n",
    "* **이상치에 민감하지 않음**\n",
    "  * Noise를 통하여 Outlier 검출 가능\n",
    "* 잡음 샘플을 구분해냄\n",
    "  * K-means나 계층 군집과 달리, 모든 샘플을 클러스터에 할당하지 않음\n",
    "#### 👎 단점\n",
    "* `epsilon`과 `min_samples` 설정에 많은 영향을 받음\n",
    "* `epsilon`을 조절하기 쉽지 않음\n",
    "  * 적절히 조절하기 위해서는 *데이터셋에 대한 도메인 지식이 필요한 경우*가 많음\n",
    "* 연산량이 많아 *속도가 느림*\n",
    "* 서로 다른 밀도 분포를 가진 데이터의 군집 분석을 잘 하지 못함 <br> <img src=\"./img/img16.webp\" width=500>\n",
    "  * *밀도가 높은 곳에 집중*하다 보니, 왼쪽에 *밀도가 낮은 곳의 데이터*를 하나의 군집으로 인식하지 못하고, *Noise point*로 구분해버림.\n",
    "* **차원의 저주**를 벗어나지 못함\n",
    "  * 모든 유클리디안 거리를 계산하는 알고리즘이 갖는 문제 <br> <img src=\"./img/img17.webp\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. Gaussian Mixture Model (GMM)\n",
    "### 🎯 Gaussian Mixture Model (GMM)\n",
    "* 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 군집화 방법\n",
    "* **모델 기반 군집화(Model-based Clustering)** 방법 중 하나임\n",
    "  * 데이터를 생성하는 통계적 모델을 가정\n",
    "  * 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화를 수행하는 접근 방식\n",
    "  * **거리 기반 군집화**와의 차이점 (ex. k-means)\n",
    "    * 각 군집을 **확률 분포(Probability Distribution)으로 간주**\n",
    "    * 전체 데이터 분포를 **이질적인 여러 개의 확률 분포의 혼합**으로 보고 모델링\n",
    "  * 데이터의 **생성 메커니즘을 모델링**할 수 있음\n",
    "### 📍 GMM의 가정\n",
    "* 관측된 데이터 포인트들은 **특정 가우시안 확률 분포**에 의해 생성되었다고 가정\n",
    "* 군집화를 적용하고자 하는 전체 데이터셋에서는 여러 개의 (다변량) 가우시안 분포가 섞여 있고, 개별 데이터는 우도(가능도, likelihood)에 따라 k개의 가우시안 분포 중 한 가지에 속함.\n",
    "  * 섞인 데이터 분포에서 각각의 가우시안 분포를 추출해 냄\n",
    "  * 각각의 분포에 기반해 군집화를 수행하게 됨\n",
    "### 📍 GMM의 진행 과정\n",
    "##### 1️⃣ 주어진 **전체 데이터셋**의 분포를 확인한다.\n",
    "\n",
    "<img src=\"./img/img18.webp\" width=400>\n",
    "\n",
    "##### 2️⃣ 전체 데이터셋은 **서로 다른 정규분포 형태의 확률분포 곡선**으로 구성되어 있다고 가정\n",
    "\n",
    "<img src=\"./img/img19.webp\" width=400>\n",
    "\n",
    "##### 3️⃣ 전체 데이터셋을 구성하는 **여러 개의 정규분포 곡선 추출**, 개별 데이터가 이 중 어떤 정규분포에 속하는지를 결정한다. 이때 **각각의 분포**가 **하나의 군집**이 된다.\n",
    "\n",
    "<img src=\"./img/img20.webp\" width=400> <br>\n",
    "\n",
    "* 전체 데이터는 3개의 정규분포로 표현될 수 있음\n",
    "* 각 데이터가 어떤 분포에 속할 확률에 따라 군집을 형성\n",
    "* ➡️ 3개의 군집 형성\n",
    "\n",
    "### 📍 모델의 파라미터 추정\n",
    "* GMM에서는 **기댓값-최대화(Expectation-Maximization, EM) 알고리즘**을 사용하여 모델의 파라미터를 추정함\n",
    "* **각 군집**의 **중심, 모양, 크기, 방향**까지 모델링 할 수 있음.\n",
    "$$\\begin{array}{l} \\text{parameter } \\theta = (u, \\Sigma, \\pi) \\\\ \\text{hyper parameter } K(\\text{분포 개수}) \\end{array}$$\n",
    "\n",
    "#### [EM 알고리즘의 단계]\n",
    "1. **Initialization**: 필요한 파라미터 $\\mu, \\sum, \\pi$에 대해 초기값을 선정한다.\n",
    "2. **E(Expectation) step**: 현재 $\\theta$를 통해 $x$가 **특정 분포(군집)에 속할 사후확률**을 계산한다.\n",
    "3. **M(Maximization) step**: 계산된 사후확률을 통해 $\\mu, \\sum, \\pi$를 다시 추정한다.\n",
    "4. 수렴 조건이 만족될 때까지 E step, M step을 반복한다.\n",
    "   \n",
    "### 📍 GMM의 장단점\n",
    "#### 👍 장점\n",
    "* k-means보다 유연하게 **다양한 데이터셋에 잘 적용**될 수 있음\n",
    "  * k-means가 잘 작동하지 않는 **타원형 분포, 중첩된 군집 구조**에서도 좋은 성능을 낼 수 있음\n",
    "#### 👎 단점\n",
    "* 군집화를 위한 **수행 시간이 오래 걸림**\n",
    "* **가정한 가우시안 분포에 맞지 않은** 데이터일 경우, **계산 복잡도가 높**아지고 **성능이 저하**될 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#FFE6E6; font-size:40px; font-weight:bold; color:#000000;\">\n",
    "# 3. 군집화 평가 방법\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "* 비지도 학습을 평가하기 위해..\n",
    "* 알고리즘 자체의 **내부 평가(Internal Evaluation) 지표**를 사용해야 함\n",
    "  * ex) `k-means`: 클래스 내 SSE(왜곡)를 사용함\n",
    "  * 군집 내부의 응집도(cohesion)와 군집 간의 분리도(separation)를 정량적으로 측정하여 **데이터 자체의 구조를 기반으로 군집 품질을 판단**함\n",
    "  * 대표적 지표: 실루엣 계수(Silhouette Coefficient)\n",
    "* 데이터셋에 정답 레이블이 존재하는 경우에도 비지도 학습 방법 적용 가능\n",
    "  * 이 경우 **외부 평가(External Evaluation) 지표**를 활용해 군집화 결과 평가 가능\n",
    "    * 실제 레이블(정답)이 존재하는 상황에서, 군집화 결과와 실제 클래스 간의 일치도를 비교함\n",
    "    * 군집화의 정확을 객관적으로 평가함\n",
    "    * 대표적 지표: ARI (Adjusted Rand Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. 실루엣 계수\n",
    "### 🎯 실루엣 계수 (Silhouette Coefficient)\n",
    "* 클러스터 내 샘플들이 얼마나 조밀하게 모여 있는지를 측정하는 도구\n",
    "* 군집의 품질을 확인할 수 있음 <br> <img src=\"./img/img21.webp\" width=400>\n",
    "### 📍 실루엣 계수 계산 과정\n",
    "1. 샘플 $\\mathbf{x}^{(i)}$와 동일한 클러스터 내 모든 다른 포인트 사이의 거리를 평균하여 **클러스터 응집력(cluster cohesion)** $a^{(i)}$를 계산한다.\n",
    "2. 샘플 $\\mathbf{x}^{(i)}$와 가장 가까운 클러스터의 모든 샘플 간 평균 거리로 최근접 클러스터의 클러스터 분리도 (custer separation) $\\mathbf{b}^{(i)}$ 를 계산한다.\n",
    "3. 클러스터 응집력과 분리도 사이의 차이를 둘 중 큰 값으로 나누어 실루엣 $s^{(i)}$ 를 다음과 같이 계산합니다.\n",
    "$$s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{\\max \\left\\{ b^{(i)}, a^{(i)} \\right\\}}$$\n",
    "\n",
    "* 실루엣 계수는 -1과 1 사이의 값을 가짐\n",
    "  * 실루엣 계수 = 0: 클러스터 응집력과 분리도가 같을 경우\n",
    "  * 실루엣 계수 1에 가까워짐: $b^{(i)} >> a^{(i)}$일 때\n",
    "\n",
    "### 📍 실루엣 계수의 시각화 예시\n",
    "* **군집화가 잘 된 경우**\n",
    "  * <img src=\"./img/img22.webp\" width=400>\n",
    "  * 실루엣 계수의 값 분포가 고름\n",
    "  * 평균 실루엣 계수가 0.7 이상임\n",
    "* **군집화가 잘 되지 않은 경우**\n",
    "  * <img src=\"./img/img23.webp\" width=400>\n",
    "  * 실루엣 계수의 값 분포가 불균형함\n",
    "  * 1번 클러스터(검정색)의 경우, 많은 데이터 포인트들의 값이 낮은 값을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Dunn Index\n",
    "### 🎯 Dunn Index\n",
    "* 클러스터 간 최소 거리(분리도)와 클러스터 내 최대 거리(응집도)의 비율을 계산해 클러스터링의 품질을 평가하는 지표\n",
    "$$\\text{Dunn Index} = \\frac{\n",
    "\\min\\limits_{i \\ne j} \\; d(C_i, C_j)}{\\max\\limits_{1 \\le k \\le K} \\; \\delta(C_k)}$$\n",
    "* $d(C_i, C_j)$ : 클러스터 $C_i$와 $C_j$ 사이의 거리\n",
    "    * 일반적으로 클러스터 중심 사이 거리 또는 두 클러스터 간의 최소 거리\n",
    "* $\\delta(C_k)$ : 클러스터 $C_k$ 내의 최대 거리\n",
    "    * = 해당 클러스터 안에서 가장 멀리 떨어진 두 점 간의 거리\n",
    "#### 해석\n",
    "* **Dunn Index 값이 클**수록 **좋을 군집화 결과**를 의미함\n",
    "  * 기본적으로 군집화는 \n",
    "    * **군집 간 거리가 클수록(분리도가 높을수록)** 좋음\n",
    "    * **군집 내부 거리가 작을수록(응집도가 높을수록)** 좋음\n",
    "### 📍 Dunn Index의 한계\n",
    "* 군집 수가 많아질수록 계산 비용 증가\n",
    "* 이상치에 민감할 수 있음"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
