{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78e1d72-7ca8-43bd-b4ed-ba7c4874e57c",
   "metadata": {},
   "source": [
    "# 1. 군집화(Clustering)\n",
    "\n",
    "## (1) 머신러닝: 비지도 학습\n",
    "\n",
    "* **머신러닝**: 인공지능의 한 분야로, **컴퓨터가 스스로 학습**할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야\n",
    "\n",
    "### [ 지도 학습과 비지도 학습 ]\n",
    "* **지도 학습**: 문제와 정답을 모두 알려주고 공부시키는 방법\n",
    "* **비지도 학습**: 답을 가르쳐주지 않고 공부시키는 방법\n",
    "\n",
    "| **정답 여부** | **방식** | **예시** |\n",
    "|--------------|---------|---------|\n",
    "| **지도 학습** | 정답(label)이 있는 데이터 사용 | 회귀, 분류 |\n",
    "| | 예측값(prediction)을 이미 만들어둔 정답과 같아지도록 기계를 학습시키는 것 | |\n",
    "| **비지도 학습** | 정답(label)이 없는 데이터 사용 | **군집화** |\n",
    "| | 데이터 속의 패턴 또는 각 데이터 간의 유사도를 기계가 학습하도록 하는 것 | |\n",
    "\n",
    "### [ 비지도 학습 ]\n",
    "* 데이터 자체에 내재된 구조를 학습하는 머신러닝 방법 \n",
    "* 비지도학습은 정답이 없는 데이터 이용\n",
    "* 패턴이 알려지지 않았거나 계속해서 변화하는 문제 해결에 유용\n",
    "* 구체적이고 좁은 범위의 문제에는 지도학습보다 불리\n",
    "* 열린 문제 해결과 지식 일반화에는 유리\n",
    "* 정답 레이블 없음 → 에이전트의 작업 명확히 정의되지 않음\n",
    "* 모델의 성능 명확히 측정 불가능\n",
    "* 표현 학습 수행으로 데이터셋의 고유 패턴 식별 가능\n",
    "\n",
    "*예시*: \n",
    "* 레이블 없는 이미지 데이터셋에서 비슷한 이미지 식별 및 그룹화 가능\n",
    "* 자동으로 '고양이', '강아지' 같은 이름 부여는 불가능\n",
    "* 그룹화된 이미지들에 더 간단히 레이블 지정 가능\n",
    "\n",
    "## (2) 군집화란?\n",
    "\n",
    "* **군집화(clustering)**: 데이터를 비슷한 특성을 가진 그룹(군집)으로 나누는 비지도 학습 기법\n",
    "* → 각 그룹 구성 파악하며 전체 데이터 구조 이해 가능\n",
    "* **군집(cluster)**: 유사한 데이터들의 집합. 서로 다른 군집에 속한 개체들은 비슷하지 않아야 함\n",
    "\n",
    "* 유사성 기반으로 개체를 함께 그룹화하는 방법\n",
    "* 하나의 관측치가 다른 관측치 또는 그룹의 데이터와 유사성 비교 작업\n",
    "\n",
    "* 분류/예측 과제에서 지도 학습 알고리즘: 독립변수와 종속변수 관계의 패턴 발견해 종속변수 예측\n",
    "* 군집화: 종속변수 없음 → 특정 목표 없이 패턴 인식\n",
    "* 사전 정의된 목표 없이 데이터셋의 고유 패턴 발견 및 추출\n",
    "* 분류/예측은 지도학습, 클러스터링은 비지도학습으로 분류\n",
    "\n",
    "> **군집화의 목표**\n",
    "> * **응집도(cohension) 최대화**: 같은 군집 내 데이터끼리 최대한 비슷하게 함\n",
    "> * **분리도(separation) 최대화**: 서로 다른 군집은 최대한 분리되도록 함\n",
    "> ⇒ 데이터 간 유사성 최대한 유지하면서 서로 다른 그룹은 구분되도록 만드는 것\n",
    "\n",
    "## (3) 군집화 과정\n",
    "\n",
    "### [ 군집화의 기본 과정 ]\n",
    "\n",
    "군집화는 기본적으로 다음 과정을 거침:\n",
    "\n",
    "**1️⃣ 피처 선택 또는 추출**\n",
    "**2️⃣ 군집화 알고리즘 선택**\n",
    "**3️⃣ 군집 유효성 검증**\n",
    "**4️⃣ 결과 해석**\n",
    "\n",
    "> **군집화에서 주요 고려사항**\n",
    "> 1. **변수 유형 이해**: \n",
    ">    * 변수 종류가 연속형/명목형인지 파악\n",
    ">    * 변수 개수와 특징 이해 필요\n",
    ">    * 변수 종류와 특징에 따라 방법론 결정\n",
    "> 2. **거리(또는 유사도) 정의와 측정**:\n",
    ">    * 군집화는 데이터 간 유사도 기반으로 수행\n",
    ">    * 거리(distance) 또는 유사도(similarity) 측정 방식이 중요\n",
    ">    * 회귀분석에서는 변수 자체가 중요했다면 군집분석에서는 거리 정의와 측정이 더 중요\n",
    "> 3. **차원 축소**:\n",
    ">    * 유사한 변수들을 묶어 차원 축소\n",
    ">    * 예: 나이와 연차는 비슷한 변수로 묶을 수 있음 → 나이 제외하고 연차만 사용하여 차원 축소 가능\n",
    "\n",
    "* 군집화는 one-shot 프로세스가 아님\n",
    "* 많은 경우 여러 번의 반복적 시도 필요\n",
    "* 피처와 군집화 스키마 선택 시 명확한 기준 없음\n",
    "* 적합한 평가 기준 선정도 하나의 문제가 될 수 있음\n",
    "\n",
    "# 2. 군집화 알고리즘\n",
    "\n",
    "## (1) 계층적 군집화(Hierarchical Clustering)\n",
    "\n",
    "- 데이터 간 유사성 기반으로 트리 구조(dendrogram) 형성\n",
    "- 상향식 또는 하향식 방식으로 군집 형성\n",
    "- 군집 개수를 사전에 설정하지 않고, 클러스터링 후 선택 가능\n",
    "\n",
    "### Dendrogram\n",
    "- 가까운 개체/군집을 점차적으로 병합하는 과정을 시각화한 트리 구조\n",
    "- 수직축(높이) = 두 군집이 병합될 때의 거리(유사성)\n",
    "\n",
    "### 계층적 군집화 유형\n",
    "- 응집형(병합) 클러스터링: 상향식 구성, 작은 군집들을 병합해 큰 군집 생성\n",
    "- 분리형(분할) 클러스터링: 하향식 구성, 하나의 군집을 분할해 여러 군집 생성\n",
    "- 두 방법 모두 proximity matrix(거리/유사도 행렬) 기반 계층적 구조로 조직화\n",
    "\n",
    "### [응집형 계층적 군집화]\n",
    "- 각 데이터포인트를 개별 클러스터로 시작\n",
    "- 병합 연산으로 클러스터 통합\n",
    "- 군집 간 거리 정의에 따라 알고리즘 결정\n",
    "\n",
    "### [거리 정의 방법]\n",
    "1. Single linkage(단일/최단 연결법)\n",
    "   - 서로 다른 군집에서 가장 가까운 두 점 사이의 거리 측정\n",
    "   - 고립된 군집 찾는데 중점, 이상치에 취약\n",
    "\n",
    "2. Complete linkage(완전/최장 연결법)\n",
    "   - 군집 간 최대 거리 측정\n",
    "   - 내부 응집성에 중점, 이상치에 민감\n",
    "\n",
    "3. Average linkage(평균 연결법)\n",
    "   - 모든 항목 간 거리 평균\n",
    "   - 이상치에 덜 민감하나 계산량 많음\n",
    "\n",
    "4. Centroid Method(중심 연결법)\n",
    "   - 두 군집 중심 간 거리 측정\n",
    "   - 새 군집 중심은 가중평균으로 계산\n",
    "   - 계산 시간 오래 걸림\n",
    "\n",
    "5. Median(중앙 연결법)\n",
    "   - 군집 내 모든 샘플의 중앙값으로 거리 정의\n",
    "   - 극단값 영향 적음, 기하학적 구조 파악 어려움\n",
    "\n",
    "6. Ward's Procedure(Ward 연결법)\n",
    "   - 군집 병합 후 SSE(오차제곱합) 증가분 최소 선택\n",
    "   - 증가량 최소화 방향으로 군집 형성\n",
    "\n",
    "### [계층적 군집화 결과 시각화]\n",
    "- 덴드로그램으로 클러스터 형성 과정 요약 확인\n",
    "- 소규모 데이터에 적합, 군집 수 탐색이나 구조 시각적 분석에 유용\n",
    "\n",
    "## (2) k-means\n",
    "\n",
    "- 데이터를 정해진 개수(K)의 그룹으로 나누는 알고리즘\n",
    "- 각 그룹의 중심점(centroid)과 거리가 가까운 데이터끼리 묶음\n",
    "- 연속적 특성의 데이터에 적합한 프로토타입 기반 군집화 방법\n",
    "\n",
    "### [k-means 알고리즘 단계]\n",
    "1. 데이터에서 랜덤하게 k개 중심점(centroid) 선택\n",
    "2. 각 표본을 가장 가까운 중심점에 할당\n",
    "   - 일반적으로 유클리디안 거리의 제곱 사용\n",
    "3. 각 클러스터에 할당된 표본들의 평균으로 중심점 이동\n",
    "4. 클러스터 할당이 변하지 않거나 조건 충족까지 2, 3 반복\n",
    "\n",
    "### 구체적 계산 과정\n",
    "- 목표: 클러스터 내 제곱 오차합(SSE) 최소화\n",
    "- 수식: SSE = Σ(데이터포인트와 클러스터 중심 간 거리의 제곱)\n",
    "\n",
    "### [k-means 장단점]\n",
    "- 장점\n",
    "  - 직관적이고 구현 쉬움\n",
    "  - 대용량 데이터에도 적용 가능\n",
    "  \n",
    "- 단점\n",
    "  - 초기 centroid 값에 민감\n",
    "  - 군집 수(k) 결정 어려움\n",
    "  - 아웃라이어에 민감\n",
    "  - 기하학적 모양의 군집 파악 어려움\n",
    "  - 빈 클러스터 문제 발생 가능\n",
    "\n",
    "### [k-means++ 알고리즘]\n",
    "- 초기 중심점을 더 효율적으로 배치하는 방법\n",
    "- 중심점들을 서로 멀리 떨어진 곳에 위치시킴\n",
    "- 기본 k-means보다 일관되고 우수한 결과 도출\n",
    "\n",
    "### [엘보우 방법(elbow method)]\n",
    "- 최적 클러스터 개수(k) 결정에 사용\n",
    "- 클러스터 수 증가에 따른 왜곡(SSE) 감소율 분석\n",
    "- 왜곡이 급격히 감소하는 지점의 k값 선택\n",
    "\n",
    "## (3) DBSCAN\n",
    "\n",
    "DBSCAN은 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘이다.\n",
    "\n",
    "### [DBSCAN 알고리즘의 주요 단계]\n",
    "\n",
    "1. 각 샘플을 핵심 샘플, 경계 샘플, 잡음 샘플 중 하나로 할당\n",
    "   - 어떤 샘플의 특정 반경 ε 안에 있는 이웃 샘플이 지정된 개수(MinPts) 이상이면 **핵심 샘플(core point)**이 됨\n",
    "   - ε 이내에 MinPts보다 이웃이 적지만 다른 핵심 샘플의 반경 ε 안에 있으면 **경계 샘플(border point)**이 됨\n",
    "   - 위 두 샘플에 해당하지 않는 다른 모든 샘플은 **잡음 샘플(noise point)**이 됨\n",
    "\n",
    "2. 개별 핵심 샘플 또는 (ε 이내에 있는 핵심 샘플을 연결한) 핵심 샘플의 그룹을 클러스터로 만듦\n",
    "\n",
    "3. 경계 샘플을 해당 핵심 샘플의 클러스터에 할당\n",
    "\n",
    "### 하이퍼 파라미터\n",
    "- eps: 데이터 포인트 사이의 최대 거리\n",
    "- min_samples: 포인트가 군집이 되기 위해 eps 거리 내 포인트가 얼마나 많아야 하는지\n",
    "  - 일반적으로 min_samples 값이 증가하면 군집 수가 감소\n",
    "\n",
    "### [DBSCAN의 장단점]\n",
    "\n",
    "**장점**\n",
    "- k-means처럼 클러스터 모양을 원형으로 가정하지 않고, 밀도에 따라 클러스터를 할당하여 임의의 기하학적 분포를 갖는 데이터도 잘 처리할 수 있음\n",
    "- 이상치에 민감하지 않음: Noise를 통하여 Outlier 검출이 가능\n",
    "- k-means나 계층 군집과 달리 모든 샘플을 클러스터에 할당하지 않음 (잡음 샘플을 구분해냄)\n",
    "\n",
    "**단점**\n",
    "- epsilon과 min_samples 설정에 많은 영향을 받음\n",
    "- epsilon을 조절하기 쉽지 않으며, 적절한 조절을 위해서는 데이터셋에 대한 도메인 지식이 필요한 경우가 많음\n",
    "- 연산량이 많아 속도가 느림\n",
    "- 다른 밀도 분포를 가진 데이터의 군집 분석을 잘 하지 못함\n",
    "- '차원의 저주'를 벗어나지 못함: 모든 유클리디안 거리를 계산하는 알고리즘이 갖는 문제\n",
    "\n",
    "## (4) Gaussian Mixture Model (GMM)\n",
    "\n",
    "GMM은 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 군집화 방법이다.\n",
    "\n",
    "모델 기반 군집화 방법 중 하나로, 데이터를 생성하는 통계적 모델을 가정하고 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화를 수행한다.\n",
    "\n",
    "### [GMM의 가정]\n",
    "- 관측된 데이터 포인트들은 특정 가우시안 확률 분포에 의해 생성되었다고 가정\n",
    "- 전체 데이터셋에는 여러 개의 가우시안 분포가 섞여있고, 개별 데이터는 우도에 따라 K개의 가우시안 분포 중 한가지에 속함\n",
    "- 섞인 데이터 분포에서 각각의 가우시안 분포를 추출해 내고, 각각의 분포에 기반해 군집화를 수행\n",
    "\n",
    "### [GMM의 진행 과정]\n",
    "1. 주어진 전체 데이터셋의 분포를 확인\n",
    "2. 전체 데이터 셋은 서로 다른 정규 분포 형태의 확률 분포 곡선으로 구성되어 있다고 가정\n",
    "3. 전체 데이터셋을 구성하는 여러 개의 정규분포 곡선 추출, 개별 데이터가 이 중 어떤 정규분포에 속하는지 결정\n",
    "\n",
    "### [모델의 파라미터 추정]\n",
    "GMM에서는 기댓값-최대화(Expectation-Maximization, EM) 알고리즘을 사용하여 모델의 파라미터를 추정한다. 이를 통해 각 군집의 중심, 모양, 크기, 방향까지 모델링을 할 수 있다.\n",
    "\n",
    "EM 알고리즘 단계:\n",
    "1. Initialization: 필요한 파라미터 u, Σ, π에 대해 초기값을 선정\n",
    "2. E(Expectation) step: 현재 θ를 통해 x가 특정 분포(군집)에 속할 사후확률을 계산\n",
    "3. M(Maximization) step: 계산된 사후확률을 통해 u, Σ, π를 다시 추정\n",
    "4. 수렴 조건이 만족될 때까지 E step, M step을 반복\n",
    "\n",
    "### GMM의 장단점\n",
    "\n",
    "**장점**\n",
    "- K-means보다 유연하게 다양한 데이터 세트에 적용 가능\n",
    "- k-means가 잘 작동하지 않는 타원형 분포나, 중첩된 군집 구조에서도 좋은 성능을 낼 수 있음\n",
    "\n",
    "**단점**\n",
    "- 군집화를 위한 수행시간이 오래 걸림\n",
    "- 가정한 분포(가우시안 분포)에 맞지 않은 데이터일 경우 계산 복잡도가 높아지고 성능이 저하될 수 있음\n",
    "\n",
    "> ### K-means vs. GMM 비교\n",
    ">\n",
    "> | | K-means: 중심 기반 | GMM: 확률 분포 기반 |\n",
    "> |---|---|---|\n",
    "> | 장점 | 개별 군집 내의 데이터가 원형으로 흩어져 있는 경우에 매우 효과적인 군집화 수행 | K-means보다 유연하게 다양한 데이터 세트에 적용 가능<br>데이터가 길쭉한 > 분포여도 준수하게 군집을 형성 가능 |\n",
    "> | 단점 | 길쭉한 방향으로 데이터가 밀접해 있을 경우, 최적의 군집화가 어려움 | 오래걸림<br>정규분포 가정에 맞아야 함 |\n",
    "\n",
    "# 3. 군집화의 평가 방법\n",
    "\n",
    "비지도 학습에서는 데이터에 레이블이 없기 때문에 지도 학습 알고리즘들의 성능 평가와 같은 방식으로 평가할 수 없다. 따라서 군집화에서는 알고리즘 자체의 내부 평가 지표를 사용해야 한다.\n",
    "\n",
    "### 1. 내부 평가 (Internal Evaluation)\n",
    "- 정답 라벨이 없는 경우: 군집 내부의 응집도와 군집 간의 분리도를 기준으로 군집 품질을 평가\n",
    "- 대표적인 지표: 실루엣 계수(Silhouette Coefficient) - 한 점이 자기 클러스터 내에서 얼마나 가깝고, 다른 클러스터와는 얼마나 멀리 떨어져 있는지 측정\n",
    "\n",
    "### 2. 외부 평가 (External Evaluation)\n",
    "- 정답 라벨이 있는 경우, 실제 라벨과 군집화 결과를 비교\n",
    "- 대표적인 지표: ARI (Adjusted Rand Index)\n",
    "  - 두 군집 결과 간의 유사도를 비교하는 지표\n",
    "  - 군집화 결과가 실제 라벨과 얼마나 일치하는지를 0과 1 사이의 범위로 정량화\n",
    "  - 값이 1에 가까울수록 실제 라벨과 군집화 결과가 비슷하다고 볼 수 있음\n",
    "\n",
    "## (1) 실루엣 계수\n",
    "\n",
    "실루엣 계수(silhouette coefficient)는 클러스터 내 샘플들이 얼마나 조밀하게 모여 있는지를 측정하는 도구로, 이를 통해 군집의 품질을 확인할 수 있다.\n",
    "\n",
    "**실루엣 계수 계산 과정**\n",
    "\n",
    "1. 샘플 x^(i)와 동일한 클러스터 내 모든 다른 포인트 사이의 거리를 평균하여 클러스터 응집력(cluster cohesion) a^(i)를 계산\n",
    "2. 샘플 x^(i)와 가장 가까운 클러스터의 모든 샘플 간 평균 거리로 최근접 클러스터의 클러스터 분리도(cluster separation) b^(i)를 계산\n",
    "3. 클러스터 응집력과 분리도 사이의 차이를 둘 중 큰 값으로 나누어 실루엣 s^(i)를 계산:\n",
    "   s^(i) = (b^(i) - a^(i)) / max{b^(i), a^(i)}\n",
    "\n",
    "실루엣 계수는 -1과 1 사이 값을 가진다:\n",
    "- 클러스터 응집력과 분리도가 같을 경우 (b^(i) = a^(i)) 실루엣 계수는 0\n",
    "- b^(i) >> a^(i)이면 이상적인 실루엣 계수인 1에 가까워짐\n",
    "\n",
    "## (2) Dunn Index\n",
    "\n",
    "Dunn Index는 클러스터 간 최소 거리(분리도)와 클러스터 내 최대 거리(응집도)의 비율을 계산해 클러스터링의 품질을 평가하는 지표이다.\n",
    "\n",
    "Dunn Index = min(i≠j) d(Ci, Cj) / max(1≤k≤K) δ(Ck)\n",
    "\n",
    "- d(Ci, Cj): 클러스터 Ci와 Cj 사이의 거리\n",
    "  - 일반적으로 클러스터 중심 사이 거리 또는 두 클러스터 간의 최소 거리\n",
    "- δ(Ck): 클러스터 Ck 내의 최대 거리\n",
    "  - 해당 클러스터 안에서 가장 멀리 떨어진 두 점 간의 거리\n",
    "\n",
    "Dunn Index 값이 클수록 좋은 군집화 결과를 의미한다. 군집화는 군집 간 거리가 크고(분리도가 높음), 군집 내부 거리는 작을수록(응집도가 높음) 좋기 때문이다.\n",
    "\n",
    "**Dunn Index의 한계**\n",
    "- 군집 수가 많아질수록 계산 비용이 증가\n",
    "- 이상치에 민감할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a074c7-2c20-415c-b75e-79b778affa52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
