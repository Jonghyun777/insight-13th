{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2efd83-edf3-4215-b473-1ce336b7a4c7",
   "metadata": {},
   "source": [
    "## 군집화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5244cd-028f-4ac6-896b-e8cb89c5a3e8",
   "metadata": {},
   "source": [
    "- ### 머신러닝:  비지도 학습\n",
    "  - 데이터 자체에 내재된 구조를 학습 (종속변수x)\n",
    "  - 모델의 성능을 명확히 측정할 수는 없음\n",
    "    - 표현 학습(representation learning or 피처 학습)을 수행함으로써 데이터셋의 패턴 식별"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117938a-f3c0-4022-9ce9-16508eac1e9e",
   "metadata": {},
   "source": [
    "- ### 군집화란?\n",
    "  - clustering\n",
    "  - 데이터를 비슷한 특성을 가진 그룹(군집)으로 나눔\n",
    "  - 군집: 유사한 데이터들의 집합. 다른 군집의 개체끼리는 비슷하지 않아야 함\n",
    "  - **유사성** 기반\n",
    "    - 하나의 관측치가 다른 관측치 or 그룹의 데이터와 얼마나 유사한지 비교\n",
    "  - 목표\n",
    "    - **응집도(cohension) 최대화**\n",
    "    - **분리도(separation) 최대화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2daf3b-66f4-4782-b5e2-5a4210961270",
   "metadata": {},
   "source": [
    "- ### 군집화 과정\n",
    "  1. 피처 선택 혹은 추출\n",
    "  2. 군집화 알고리즘 선택\n",
    "  3. 군집 유효성 검증\n",
    "  4. 결과 해석\n",
    "  - 주요 고려 사항\n",
    "    - 변수 유형 이해\n",
    "      - 변수가 명목형인지 연속형인지? 변수 개수와 특징에 대한 이해\n",
    "        - 이에 따라 사용할 방법론을 결정\n",
    "        - *변수=피처=컬럼=속성*\n",
    "    - 거리(유사도) 정의와 측정\n",
    "      - 어떤 방식으로 측정하는지? 가 중요\n",
    "      - 거리를 측정하는 방법이 변수의 특성과 관계있음\n",
    "    - 차원 축소\n",
    "      - 유사한 변수들을 묶어서 차원 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0953b-5d6c-4d44-aa81-de144fb7ec42",
   "metadata": {},
   "source": [
    "## 군집화 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f6966-7ee0-40a2-bfb3-a2d0c2657b85",
   "metadata": {},
   "source": [
    "- ### 계층적 군집화 (Hierarchical Clustering)\n",
    "  - 데이터 간의 유사성을 기반으로 트리 구조(dendrogram)를 형성하여, 상향식 or 하향식 방식으로 군집을 형성해 나가는 방법\n",
    "  - 관측치를 사용해 덴드로그램을 만듦\n",
    "    - 덴드로그램의 수직출(높이)를 통해 두 군집 간 거리(유사성)을 볼 수 있음\n",
    "  - 군집의 개수 사전에 설정x. 클러스터링 이후 원하는 군집의 개수 선택\n",
    "  - 상향식(응집형 계층적 군집화 or 병합 클러스터링)\n",
    "    - 작은 군집들을 병합해나가며 더 큰 군집 만듦\n",
    "    - 합치기\n",
    "    - 각 샘플에서 하나의 클러스터만 남을때까지 가장 가까운 클러스터를 병합\n",
    "  - 하향식(분리형 계층적 군집화 or 분할 클러스터링)\n",
    "    - 하나의 군집을 분할해가며 여러 군집을 만듦\n",
    "    - 쪼개기\n",
    "    - 전체 샘플에서 유사성이 낮은 데이터들을 더 작은 클러스터로 나눔\n",
    "  - 결과가 일반적으로 이진트리 or 덴드로그램으로 표현됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68769a6a-f076-4f1f-892b-416ca22d41c8",
   "metadata": {},
   "source": [
    "- 응집형 계층적 군집화\n",
    "  - 각각 1개의 데이터포인트만을 포함하는 N개의 클러스터에서 시작\n",
    "    - *데이터포인트: 하나의 행, 하나의 관측값*\n",
    "  - 수 회의 병합 연산을 수행해 최종적으로 모든 데이터포인트들이 같은 그룹에 속하게 됨\n",
    "    - **가까운 군집 간 병합이 일어남**\n",
    "  - 군집 간의 거리를 어떻게 정의하느냐?에 따라 알고리즘이 달라짐\n",
    "  - 기본적인 알고리즘으로 단일 연결(single linkage)과 완전 연결(complete linkage)이 있음\n",
    "    - 단일 연결: 클러스터 쌍에서 가장 비슷한 샘플 간 거리를 계산, 이후 거리가 가장 작은 두 클러스터 병함\n",
    "    - 완전 연결: 가장 비슷하지 않은 샘플과 비교해 병합 수행\n",
    "  - *하나의 가계도를 만들어 나간다고 이해하면 편함*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ec0d4-2e9c-46ac-9d50-f6f292d44302",
   "metadata": {},
   "source": [
    "- 6가지 거리 정의 방법\n",
    "  1. Single linkage(단일 연결법/최단 연결법)\n",
    "     - 서로 다른 군집에서 가장 가까운 두 점 사이의 거리를 측정\n",
    "     - **고립된 군집을 찾는데 중점**을 두고 있음\n",
    "     - 이상치에 약함 (no robust)\n",
    "       - 하나의 이상치가 기준이 되어 다른 군집과의 거리가 너무 가깝게 계산될 수 있음\n",
    "       - -> 연쇄작용까지 일어날 수 있음\n",
    "  2. Compelete linkage(완전 연결법/최장 연결법)\n",
    "     - 각각의 군집에서 제일 먼 점 끼리 거리를 계산함. (거리의 최댓값)\n",
    "     - **군집들의 내부 응집성에 중점**을 둠\n",
    "     - 얘도 no robust\n",
    "  3. Average linkage(평균 연결법)\n",
    "     - 모든 항목에 대한 거리 평균\n",
    "     - 계산량이 불필요하게 많지만 robust함\n",
    "  4. Centroid Method(중심 연결법)\n",
    "     - 두 군집의 중심 간의 거리를 측정\n",
    "     - 두 군집이 결합될 때 새로운 군집의 평균은 가중 평균을 통해 구함\n",
    "     - 시간 오래 걸림\n",
    "  5. Median(중앙 연결법)\n",
    "     - 군집 내 모든 샘플의 중앙값으로 거리 계산\n",
    "     - robust\n",
    "     - 기하학적 구조 파악 어려움\n",
    "  6. Ward's Procedure(Ward 연결법)\n",
    "      - 두 군집이 병합한 이후 SEE가 가장 적게 증가하는 경우를 선택함\n",
    "      - 결합 이후 **분산의 증가량이 가장 적은** 클러스터들을 병합\n",
    "        - 응집도를 최대한 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db7b77-78e2-4552-af6b-e7f5489eb240",
   "metadata": {},
   "source": [
    "- 응집형 계층 군집화 (완전 연결 방식)\n",
    "  1. 모든 샘플의 거리 행렬 계산\n",
    "  2. 모든 데이터포인트를 단일 클러스터로 변환\n",
    "  3. 가장 비슷하지 않은 샘플 사이 거리에 기초해 가장 가까운 두 클러스터 병합\n",
    "  4. 유사도 행렬 업데이트\n",
    "  5. 하나의 클러스터로 남을때까지 2~4번 과정 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebe1f8-d951-42bc-8236-117a6add700b",
   "metadata": {},
   "source": [
    "- 활용\n",
    "  - 계산량이 많기 때문에, 소규모 데이터에 대해 군집 수를 탐색하거나 클러스터링의 전반적인 구조를 시각적으로 분석할 때 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885f9a2-9579-4209-a8a6-d92020a159bc",
   "metadata": {},
   "source": [
    "- ### k-means\n",
    "  - 데이터를 정해진 개수(k)의 그룹으로 나누되, 각 그룹의 중심점(centroid)과의 거리가 가장 가까운 데이터끼리 묶음\n",
    "    - 연속적인 특성을 갖는 데이터 샘플 중에서 **centroid**를 선택\n",
    "  - 가장 일반적으로 사용되는 알고리즘\n",
    "  - 참고;프로토타입 기반 군집화\n",
    "    - 각 클러스터가 하나의 프로토타입(대표값)으로 표현됨\n",
    "    - 연속형 -> 센트로이드(평균), 범주형 -> 메도이드(가장 대표 or 자주등장 값)\n",
    "  - 단계\n",
    "    1. 랜덤하게 k개의 centriod를 초기 클러스터 중심으로 선택\n",
    "    2. 각 표본들을 가장 가까운 중심점에 할당\n",
    "       - 이 때, 거리는 유클리디안 거리의 제곱 사용\n",
    "       - 유클리디안 거리를 사용하기 때문에 미리미리 스케일링 or 표준화를 해줘야 함\n",
    "    3. 각 클러스터당 **할당된 표본들의 평균이 되는 지점에 centriod를 이동**시킴\n",
    "    4. 클러스터 할당이 변하지 않거나 특정 반복 횟수에 도달할때까지 2~3번 과정 반복\n",
    "  - 구체적인 계산 과정\n",
    "    - 각 데이터포인트들(x_i)과 centriod(u(j))이 거리를 최소로 만드는 j번째 군집을 찾아냄\n",
    "    - k-means는 클러스터 내 제곱 오차합(SSE)를 최소화하는 방향으로 중심점을 반복적으로 이동시킴\n",
    "    - 사용자가 직접 클러스터의 개수(k)를 지정해줘야함\n",
    "    - 원형 클러스터를 구분하는 데 뛰어남\n",
    "    - 적절한 k값을 찾기 위해 엘보우 방법, 실루엣 계수 등 활용 가능\n",
    "  - 장점\n",
    "    - 직관적. 구현 easy\n",
    "    - 대용량 데이터도 ok\n",
    "  - 단점\n",
    "    - 초기 centroid에 민감함\n",
    "    - k값 결정 어려움\n",
    "    - 아웃라이어에 민감 (no robust)\n",
    "    - 기하학적인 모양의 군집 파악 어려움\n",
    "  - k-means 알고리즘: 빈 클러스터 문제\n",
    "    - 비어있는 클러스터가 존재할 수 있음\n",
    "    - 사이킷런 사용하면 ㄱㅊ\n",
    "  - k-means++ 알고리즘\n",
    "     - 초기 centriod 랜덤 할당으로 인한 단점 해결법\n",
    "       - 여러번 반복해서 제일 성능 좋은것 선택\n",
    "       - or k-means++사용해서 **초기 중심점들을 서로 멀리 떨어진 곳에서 시작**하도록 함\n",
    "    - 초기화 과정\n",
    "      1. k개의 중심점을 저장할 빈 집합 M 초기화\n",
    "      2. 첫 중심점 랜덤 선택하고 M에 할당\n",
    "      3. M에 없는 샘플에 대해 M에 있는 중심점까지의 최소 제곱 거리 찾기ㅣ\n",
    "      4. 가중치가 적용된 확률분포를 이용해 다음 중심점 랜덤하게 선택\n",
    "         - 멀리 떨어진 데이터포인트일수록 중심점으로 선택될 확률이 높음\n",
    "      5. k개의 중심점 선택할때까지 3~4번 과정 반복\n",
    "      6. 이후 k-means 알고리즘 수행\n",
    "  - 엘보우 방법(elbow method)\n",
    "     - k값에 따라 군집 품질이 달라지는 문제 해결\n",
    "     - 직관적으로, k 증가할수록 왜국 줄어듦 (샘플과 중심점과의 거리 짧아짐)\n",
    "     - 왜곡이 빠르게 감소하는 지점의 k값 찾기 -> 그래프 통해 찾아서 적절한 값을 k에 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ccb93-6281-4054-8bdd-1a4e34b7d0a1",
   "metadata": {},
   "source": [
    "- ### DBSCAN\n",
    "  - **밀도가 높은 지역의 데이터를 하나의 군집으로** 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 알고리즘\n",
    "  - 알고리즘의 주요 단계\n",
    "    1. 조건에 따라 각 샘플을 핵심샘플, 경계샘플, 잡음샘플중 하나에 할당\n",
    "      - 특정 eps 안에 있는 샘플이 Minpts이상 -> core point\n",
    "      - eps 이내의 이웃이 Minpts보다 적지만 다른 core point의 반경 eps 안에 있음 -> border point\n",
    "      - else: noise point\n",
    "    2. 개별 핵심 샘플 또는 (eps 이내에 있는 핵심 샘플을 연결한) 핵심 샘플의 그룹을 클러스터로 만듦\n",
    "    3. 경계 샘플을 해당 핵심 샘플의 클러스터에 할당\n",
    "    - eps, min_samples는 하이퍼파라미터임\n",
    "    - 일반적으로 min_samples가 증가하면 군집 수 감소\n",
    "    - 반경 eps 내에 기준이 되는 데이터포인트 포함한 데이터포인트의 개수>=min_samples이면 군집으로 인식\n",
    "   - 장점\n",
    "     - 밀도에 따라 임의의 기하학적 분포를 갖는 데이터도 잘 처리함\n",
    "     - robust함 (noise를 통해 이상치 검출)\n",
    "     - 모든 샘플을 클러스터에 할당하지 않음 (noise sample 구분)\n",
    "   - 단점\n",
    "     - epsilon과 min_samples 설정에 많은 영향 받음\n",
    "     - eps 조절 쉽지 않음. 도메인 지식 필요\n",
    "     - 연산량이 많아 속도 느림\n",
    "     - 다른 밀도를 가진 데이터의 군집 분석을 잘 하지 못함\n",
    "       - 밀도가 낮은 군집을 noise 처리할 수도\n",
    "     - 차원의 저주를 벗어나지 못함\n",
    "       - 같은 양의 데이터가 고차원으로 이동할수록 데이터의 밀도가 매우 낮아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a337cf9-f788-473a-b5e8-f57a44df3d1a",
   "metadata": {},
   "source": [
    "- ### Gaussian Mixture Model\n",
    "  - 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 방법\n",
    "  - 통계적 모델을 가정하고, 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화 수행\n",
    "  - 각 군집을 확률분포로 가정, 전체 데이터 분포를 여러 개의 확률분포의 혼합으로 보고 모델링\n",
    "  - 데이터의 생성 매커니즘을 모델링할 수 있음\n",
    "  - 가정\n",
    "    - 데이터 포인트들은 특정 가우시안 확률 분포에 의해 생성되었다고 가정\n",
    "    - 전체 데이터셋에는 여러 개의 가우시안 분포가 섞여있고, 개별 데이터는 우도(가능도)에 따라 k개의 가우시안 분포 중 하나에 속함\n",
    "    - **섞인 데이터 분포에서 각각의 가우시안 분포를 추출해내고, 각각의 분포에 기반하여 군집화 수행**\n",
    "  - 확률 vs 우도\n",
    "    - 확률: 고정된 확률분포에서 어떠한 관측값이 나타나는지에 대한 확률\n",
    "    - 우도: 고정된 관측값이 어떠한 확률분포에서 어느 정도의 확률로 나타나는지에 대한 확률\n",
    "      - 특정 모델이 이 데이터를 만들어냈을 확률\n",
    "  - 과정\n",
    "    1. 전체 데이터셋의 분포 확인\n",
    "    2. 전체 데이서텟은 서로 다른 정규분포 형태의 확률 분포 곡선으로 구성되어있다고 가정\n",
    "    3. 여러 개의 정규분포 곡선 추출, 각각의 데이터셋이 해당하는 분포(군집) 결정\n",
    "  - 파라미터 추정\n",
    "    1. initalization: 필요한 파라미터 초기화\n",
    "    2. e step: 현재의 파라미터를 통해 데이터포인트가 특정 분포에 속할 사후 확률 계산\n",
    "    3. m step: 계산된 사후 확률을 통해 파라미터 수정\n",
    "    4. 수렴 조건 만족될때까지 e,m step 반복\n",
    "  - 장점\n",
    "    - 유연함\n",
    "  - 단점\n",
    "    - 수행 시간 오래걸림\n",
    "    - 가정한 분포가 맞지 않을 경우 성능 저하 될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5d844-9274-4a8e-a739-4bc13daa65a6",
   "metadata": {},
   "source": [
    "## 군집화의 평가 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7d7a6-b31a-4ec8-b3f5-f85048219f47",
   "metadata": {},
   "source": [
    "- 답(test_data)가 없기 때문에 지도학습의 평가 방법을 사용할 수 없음\n",
    "- 자체의 내부 평가 지표 활용\n",
    "- **군집 내부의 응집도 & 군집 간의 분리도**를 정량적으로 측정하여 데이터 자체의 구조를 기반으로 품질 판단\n",
    "- 정답 레이블이 존재하는 경우에도 이 방법을 활용할 수 있다 (외부 평가)\n",
    "  - 군집화 결과 vs 실제 클래스 간의 일치도 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5e8dc-653f-4649-a685-f5bbb5ef8433",
   "metadata": {},
   "source": [
    "- 실루엣 계수\n",
    "  - 클러스터 내 샘플들이 얼마나 조밀하게 모여있는지 측정\n",
    "  - 계산 과정\n",
    "    - 특정 샘플 x_i과 그 샘플과 같은 클러스터 내의 다른 포인트 간의 거리를 평균내서 클러스터 응집력 a 계산\n",
    "    - x_i와 가장 가까운 클러스터의 모든 샘플 간 평균거리로 최근접 클러스터의 클러스터 분리도 b 계산\n",
    "    - s=(b-a)/(max(b,a))\n",
    "    - -1~1사이의 값이 나옴\n",
    "    - 이상적인 경우 1에 가깝게 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca816de-9666-4aff-bf92-38585ec54cb6",
   "metadata": {},
   "source": [
    "- Dunn Index\n",
    "  - 클러스터간 최소 거리(분리도)와 클러스터 내 최대 거리(응집도)의 비율 계산\n",
    "  - (두 클러스터 간의 최소 거리)/(해당 클러스터 내의 가장 먼 포인트들의 거리)\n",
    "    - 최악의 분리도/최악의 응집도 비율 계산?\n",
    "  - 값이 클수록 좋음 -> 분리도 높고 응집도 높다는 얘기\n",
    "  - 한계\n",
    "    - 군집 수 많아질수록 계산 비용 증가\n",
    "    - no robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f54407-e1f7-42ab-a45b-9697fae243fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
