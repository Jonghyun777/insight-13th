{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 군집화\n",
    "## (1) 머신러닝 : 비지도 학습\n",
    "- 비지도 학습 = 패턴이 알려지지 않았거나 계속해서 변화하는 문제를 해결할 때 유용함. \n",
    "- 모델의 성능을 명확히 측정할 수 없다 - 정답 레이블이 없기 때문. \n",
    "\n",
    "## (2) 군집화란\n",
    "- 데이터를 비슷한 특성을 가진 그룹(군집)으로 나누는 비지도 학습 기법\n",
    "- 군집 = 유사한 데이터들의 집합\n",
    "- 유사성을 기반으로 개체들을 그룹화하는 방식. 얼마나 유사한지 비교\n",
    "### 군집화의 목표\n",
    "- 응집도 최대화 : 같은 군집에 속하는 데이터끼리는 최대한 비슷하도록 함\n",
    "- 분리도 최대화 : 서로 다른 군집은 최대한 분리되도록 함\n",
    "\n",
    "## (3) 군집화 과정\n",
    "1. 피처 선택 또는 추출\n",
    "2. 군집화 알고리즘 선택\n",
    "3. 군집 유효성 검증\n",
    "4. 결과 해석\n",
    "-> 군집화는 여러 번의 반복적인 시도를 요구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 군집화 알고리즘\n",
    "## (1) 계층적 군집화\n",
    "- 계층적 군집화 : 데이터 간의 유사성을 기반으로 **트리 구조**를 형성하며, 상향식 또는 하향식 방식으로 군집을 형성해 나가는 방법\n",
    "- 군집의 개수는 클러스터링이 종료된 후에 선택하는 구조임\n",
    "- 응집형 계층적 군집화 : 상향식, 작은 군집들 (처음에는 각 샘플)을 병합해가며 더 큰 군집을 만드는 것\n",
    "- 분리형 계층적 군집화 : 하향식, 하나의 군집(전체 샘플)을 분할해가며 여러 군집을 만드는 것\n",
    "\n",
    "**[응집형 계층적 군집화]** <br>\n",
    "=> 각각 1개의 데이터포인트만을 포함하는 N개의 클러스터에서 시작, 병합 연산을 통해 이 클러스터들을 합쳐 더 큰 클러스터로 만든다. <br>\n",
    "응집형 계층 군집화의 거리 정의 알고리즘:\n",
    "- 단일 연결 : 클러스터 쌍에서 **가장 비슷한 샘플 간 거리**를 계산한 뒤, 거리가 가장 작은 두 클러스터를 **병합**\n",
    "- 완전 연결 : 가장 비슷하지 **않은** 샘플을 비교해 병합 수행\n",
    "\n",
    "**[6가지 거리 정의 방법]** <br>\n",
    "1. single linkage - 서로 다른 군집에서 가장 가까운 두 점 사이의 거리를 군집 간 거리로 측정하는 방법. => 이상치에 취약하다.\n",
    "2. complete linkage : 군집에서 나타날 수 있는 가장 먼 거리 (최댓값)으로 측정, 군집들의 내부 응집성에 중점을 둠. => 이상치에 취약\n",
    "3. average linkage : 모든 항목에 대한 거리 평균 계산 => 계산이 많아지지만 이상치에는 덜 민감함\n",
    "4. centroid method : 두 군집의 중심 간 거리 측정. => 중싱을 측정하는 데에 시간 소요\n",
    "5. median : 모든 샘플의 중앙값으로 정의. \n",
    "6. ward's procedure : 군집의 병합 후 군집 내 SSE의 증가분 최소인 것을 선택. 증가량이 작아지는 방향으로 군집을 형성해 나가는 방법. \n",
    "모든 샘플의 거리 행렬 계산 -> 모든 데이터 포인트를 단일 클러스터로 표현 -> 가장 비슷하지 않은 샘플 사이 거리에 기초하여 가장 가까운 두 클러스터 병함 -> 유사도 행렬 업데이트 -> 반복\n",
    "\n",
    "**[계층적 군집화 결과 시각화하기]** <br>\n",
    "- 덴드로그램을 이용한 시각화\n",
    "- 계층적 군집화는 계산량 많음 => 소규모 데이터에 대해 군집 수를 탐색 + 클러스터링의 전반적 구조를 볼 때 사용하기 좋음\n",
    "\n",
    "## (2) k-means\n",
    "= 데이터를 정해진 개수의 그룹으로 나누되, 각 그룹의 중심점과의 거리가 가장 가까운 데이터끼리 묶는 군집화 알고리즘 <br>\n",
    "= 군집 중심점을 선택해, 중심에 가장 가까운 데이터들을 묶어나가는 방식임\n",
    "\n",
    "**[k-means 알고리즘의 주요 단계]**\n",
    "1. 데이터 표본 중에서 랜덤한 k개의 중심점을 선택\n",
    "2. 각 표본들을 가장 가까운 중심점에 할당\n",
    "3. 각 클러스터의 데이터 평균을 계산하여 중심점을 이동시킴\n",
    "4. 클러스터 할당이 변하지 않을 때까지 / 사용자가 지정한 허용 오차 또는 최대 방복횟수에 도달할 때까지 반복\n",
    "\n",
    "**[엘보우 방법]** <br>\n",
    "= 클러스터 수(k)를 증가시키면서 SSE의 감소율이 급격히 완화되는 지점을 최적의 k로 선택하는 기법\n",
    "\n",
    "## (3) DBSCAN\n",
    "= 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘\n",
    "\n",
    "**[DBSCAN 알고리즘의 주요 단계]**\n",
    "1. 조건에 따라 각 샘플을 핵심/경계/잡음 샘플 중 하나에 할당\n",
    "    - 핵심 샘플 : 샘플으로부터 특정 반경에 있는 이웃 샘플이 일정 개수 이상일 떄\n",
    "    - 경계 샘플 : 반경 이내에 핵심 샘플보다는 이웃 샘플이 적지만 다른 핵심 샘플의 반경 안에 있는 경우\n",
    "    - 잡음 샘플 : 핵심/경계 샘플이 아닌 것\n",
    "2. 개별 핵심 샘플 또는 핵심 샘플의 그룹을 클러스터로 만든다.\n",
    "3. 경계 샘플을 해당 핵심 샘플의 클러스터에 할당\n",
    "\n",
    "** 하이퍼파라미터 : eps(최대 거리), min_samples (spe 거리 내 포인트가 얼마나 많아야 하는지)\n",
    "\n",
    "**[DBSCAN의 장단점]** <br>\n",
    "- 클러스터 모양이 원형이 아니더라도 **밀도**에 따라 클러스트를 할당함. 임의의 기하학적 분포를 갖는 데이터도 잘 처리할 수 있다.\n",
    "- 이상치에 민감 X\n",
    "- 모든 샘플을 클러스터에 할당하지 않음 = 잡음 샘플을 구분해냄\n",
    "- epsilon, min_samples 설정에 많은 영향을 받음 (하이퍼파라미터)\n",
    "- eps를 조절하기 쉽지 않음, 적절하게 조정하기 위해서는 데이터셋에 대한 도메인 지식이 필요함\n",
    "- 다른 밀도 분포를 가진 데이터의 군집 분석을 잘 하지 못함\n",
    "- 차원의 저주 문제 (유클리드 거리 계산하기 때문)\n",
    "\n",
    "## (4) Gaussian Mixture Model\n",
    "= GMM, 모델 기반 군집화. 데이터를 생성하는 통계적 모델을 가정하고, 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화를 수행함. <br>\n",
    "=> 각 군집 = 확률 분포 / 전체 데이터 분포 = 여러 개의 확률 분포의 혼합 임을 가정하고 모델링\n",
    "=> 데이터의 생성 매커니즘을 모델링할 수 있다. \n",
    "\n",
    "**[GMM의 가정]**\n",
    "- 관측된 데이터 포인트들은 특정 가우시안 확률 분포에 의해 상성됨\n",
    "- 군집화를 적용하고자 하는 전체 데이터셋에는 다변량 가우시안 분포가 섞어있고, 개별 데이터는 우도(가능도)에 따라 K개의 가우시안 분포 중 한가지에 속한다.\n",
    "\n",
    "**[GMM의 진행 과정]**\n",
    "1. 전체 데이터셋의 분포 확인\n",
    "2. 전체 데이터셋 = 서로 다른 정규 분포 형태의 확률 분포 곡선으로 구성되어 있다고 가정\n",
    "3. (2)에서 가정한 여러 개의 정규분포 곡선 추출, 개별 데이터가 어떤 정규분포에 속하는지 결정 => 각각의 분포는 하나의 군집이 됨\n",
    "\n",
    "**[모델의 파라미터 추정]**\n",
    "- GMM은 기댓값-최대화 알고리즘을 사용하여 모델의 파라미터를 추정한다.\n",
    "- 파라미터 추정 순서\n",
    "    1. initialization - 초기값 선정\n",
    "    2. expectation setp - x가 특정 분포/군집에 속할 사후확률 계산\n",
    "    3. maximization step - 계산한 사후확률을 기반으로 다른 변수들을 다시 추정\n",
    "    4. e-step, m-step 반복\n",
    "\n",
    "**[GMM의 장단점]**\n",
    "- k-means보다 유연함 (다양한 데이터셋에 적용 - 타원형 분포, 중첩된 군집 구조 등)\n",
    "- 수행 시간이 오래걸림\n",
    "- 가우시안 분포 가정이 맞지 않는 데이터의 경우 복잡도 높아짐, 성능 저하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 군집화의 평가 방법\n",
    "- 비지도 학습 = 레이블/정답 없음 = 지도학습의 평가 방법을 적용할 수 없음\n",
    "- 내부 평가 지표 : 군집 내부의 응집도, 군집 간 분리도를 정량적으로 측정하여 군집 품질을 판단함\n",
    "- 외부 평가 지표 : 정답 레이블이 존재하면서 비지도학습을 적용한 경우, 군집화 결과와 실제 클래스의 일치도를 비교함\n",
    "\n",
    "## (1) 실루엣 계수\n",
    "= 클러스터 내 샘플들이 얼마나 조밀하게 모여있는지 측정\n",
    "\n",
    "**[실루엣 계수 계산 과정]**\n",
    "1. 샘플과 동일한 클러스터 내 다른 포인트 사이의 거리 평균 -> 클러스터 응집력 $\\mathbf{a}^{(i)}$\n",
    "2. 샘플과 가장 가까운 클러스터의 모든 샘플 간 평균 거리 -> 클러스터 분리도 $\\mathbf{a}^{(i)}$\n",
    "3. (1), (2)의 차이를 둘중 큰 값으로 나눔\n",
    "\n",
    "- -1 < 실루엣 계수 < 1, 1에 가까울수록 이상적\n",
    "- $b^{(i)} >> a^{(i)}$ 일수록 1에 가까워짐\n",
    "\n",
    "- 군집화가 잘 된 경우: 시각화 시 $s^{(i)}$의 값 분포가 고름, $s^{(i)}$의 평균이 0.7 이상\n",
    "\n",
    "## (2) Dunn Index\n",
    "= 클러스터 간 최소 거리(분리도)와 클러스터 내 최대 거리(응집도)의 비율을 계싼해 클러스터링의 품질을 평가하는 지표 <br>\n",
    "$\\text{Dunn Index} = \\frac{\n",
    "\\min\\limits_{i \\ne j} \\; d(C_i, C_j)}{\\max\\limits_{1 \\le k \\le K} \\; \\delta(C_k)}$ <br>\n",
    "- $d(C_i, C_j)$ : 클러스터 $C_i$, $C_j$ 사이의 거리\n",
    "- $\\delta(C_k)$ : $C_k$ 내의 최대 거리\n",
    "<br>\n",
    "- dunn index 값이 클수록 군집화 결과가 좋음\n",
    "- 다만 : 군집 수가 많아질수록  계산 비용이 증가함, 이상치에 민감할 수 있음. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
