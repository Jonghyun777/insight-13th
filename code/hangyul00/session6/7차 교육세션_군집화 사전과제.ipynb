{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ce2c4c-5a16-40ef-ba6a-5271f309e89e",
   "metadata": {},
   "source": [
    "# 1. 군집화(clustering)\n",
    "\n",
    "## 1. 머신러닝:비지도학습\n",
    "머신러닝: 컴퓨터가 스스로 학습할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야\n",
    "\n",
    "비지도학습\n",
    "- 정답이 없는 데이터를 이용하는 것. 따라서 패턴이 알려지지 않았거나 계속 변화하는 문제를 해결할 때 유용하다.\n",
    "- 정답 레이블이 없으므로 모델의 성능을 명확하게 측정할 수는 없다.\n",
    "- 표현학습을 수행함으로써 데이터셋의 고유 패턴을 식별할 수 있다.\n",
    "\n",
    "## 2. 군집화란?\n",
    "군집화\n",
    "- 데이터를 비슷한 특징을 가진 그룹으로 나누는 비지도 학습 기법 (이때, 서로 다른 군집에 속한 개체는 비슷하지 않아야 한다.)\n",
    "- 유사성을 기반으로 개체를 함께 그룹화하는 방법\n",
    "- 군집화에서는 종속변수가 없기 때문에 특정한 목표 없이 패턴을 인식하고, 그 패턴을 발견하고 추출하는 도구이다.\n",
    "\n",
    "군집화의 목표\n",
    "1) 응집도 극대화: 같은 군집에 속하는 데이터끼리 최대한 비슷하도록 하기\n",
    "2) 분리도 극대화: 다른 군집은 최대한 분리하기\n",
    "\n",
    "## 3. 군집화 과정\n",
    "1) 피처 선택 또는 추출\n",
    "2) 군집화 알고리즘 선택\n",
    "3) 군집 유효성 검증\n",
    "4) 결과 해석\n",
    "\n",
    "군집화에서 고려해야 하는 사항\n",
    "1) 변수 유형 이해: 변수가 연속형인지, 명목형인지, 또한 변수의 개수와 특징에 대한 이해가 필요함\n",
    "2) 거리 정의와 측정: 군집화는 데이터 간의 유사도를 기반으로 이뤄지기 때문에 어떤 방식으로 거리를 측정하는 지가 중요\n",
    "3) 차원 축소: 유사한 변수들을 묶어서 차원을 축소 (비슷한 변수를 묶고, 하나만 사용하여 차원을 축소할 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c7c21-0694-4530-9821-e8346b624742",
   "metadata": {},
   "source": [
    "# 2. 군집화 알고리즘\n",
    "\n",
    "## 1. 계층적 군집화\n",
    "\n",
    "계층적 군집화: 데이터 간의 유사성을 기반으로 트리 구조를 형성하며, 상향식 또는 하향식 방식으로 군집을 형성해 나가는 방법 (이때, 알고리즘은 데이터셋의 관측치를 사용해 덴드로그램을 만든다.)\n",
    "\n",
    "** 덴드로그램: 가까운 두 개체 또는 군집을 점차적으로 병합해가는 과정을 시각적으로 표현한 트리 구조\n",
    "덴드로그램에서 두 데이터가 연결되는 수직축의 높이가 짧으면 분포가 가깝고, 수직축의 높이가 높을수록 분포가 멀다고 해석할 수 있음.\n",
    "\n",
    "계층적 군집화에서는 군집의 개수를 사전에 정하지 않고, 클러스터링이 종료된 후 원하는 군집의 개수를 선택함!\n",
    "\n",
    "응집형 계층적 군집화: 트리 구조를 상향식으로 구성하는 경우 (작은 군집을 병합해가며 더 큰 군집을 형성)\n",
    "분리형 계층적 군집화: 트리 구조를 하향식으로 구성하는 경우 (하나의 군집을 분할해가며 여러 군집 형성)\n",
    "\n",
    "병합 클러스터링: 먼저 각 샘플이 독립적인 클러스터가 되고, 이후 하나의 클러스터가 남을 때까지 가장 가까운 클러스터를 병합\n",
    "분할 클러스터링: 전체 샘플을 포함하는 하나의 클러스터에서 시작하여 유사성이 낮은 데이터들을 더 작은 클러스터로 나누는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f1d10-2d65-4795-831e-91a51236459f",
   "metadata": {},
   "source": [
    "[응집형 계층적 군집화]\n",
    "\n",
    "응집형 계층적 군집화\n",
    "- 각각 1개의 데이터포인트만을 포함하는 N개의 클러스터에서 시작하고, 여러 차례의 병합 연산을 통해 이 클러스터들을 합쳐서 더 큰 클러스터로 만든다.\n",
    "- 계속해서 병합 연산을 하면 최종적으로 모든 데이터포인트들이 같은 그룹에 속하게 될 것인데, 이때 가까운 군집 간 병합이 일어나기 때문에 군집 간의 거리를 어떻게 정의하느냐에 따라 알고리즘이 달라진다.\n",
    "\n",
    "** 응집형 계층 군집화의 알고리즘\n",
    "1) 단일연결: 클러스터 쌍에서 가장 비슷한 샘플 거리를 계산하고 이후 이 거리가 가장 작은 두 클러스터를 병합\n",
    "2) 완전 연결: 가장 비슷한 샘플이 아니라, 가장 비슷하지 않은 샘플을 비교해 병합을 수행\n",
    "\n",
    "[6가지 거리 정의 방법]\n",
    "1) 단일 연결법/최단 연결법: 서로 다른 군집에서 가장 가까운 두 점 사이의 거리를 군집 간의 거리로 측정하는 방법으로 고립된 군집을 찾는데 중점을 두며, 이상치에 취약함.\n",
    "2) 완전 연결법/최장 연결법: 군집에서 하나씩 관측값을 뽑았을 때 나타날 수 있는 거리의 최댓값으로 측정하여 가장 유사성이 큰 군집으로 병합해 나가는 방법으로 군집들의 내부 응집성에 중점을 둔 방법이며 이상치에 민감함.\n",
    "3) 평균 연결법: 모든 항목에 대한 거리 평균을 구하고, 이상치에 덜 민감함.\n",
    "4) 중심 연결법: 두 군집의 중심 간의 거리를 측정하고, 두 군집이 결합될 때 새로운 군집의 평균은 가중평균을 통해 구하며, 시간이 오래 걸림.\n",
    "5) 중앙 연결법: 모든 샘플의 중앙값으로 정의하고, 극단값에 영향을 덜 받지만 기하학적 구조를 파악하기는 어려움.\n",
    "6) Ward 연결법: 군집의 병합 후, 군집 내 SSE(오차제곱합)의 증가분이 최소인 것을 선택함. 두 군집이 합해지면 병합된 군집의 SSE는 병합 이전 각 군집의 SSE 합보다 커지게 되는데, 그 증가량이 가장 작아지는 방향으로 군집을 형성해 나가는 방법.\n",
    "\n",
    "** 완전 연결 방식을 사용한 응집형 계층 군집화의 과정\n",
    "1) 모든 샘플의 거리 행렬 계산\n",
    "2) 모든 데이터 포인트를 단일 클러스터로 표현\n",
    "3) 가장 비슷하지 않은 샘플 사이 거리에 기초하여 가장 가까운 두 클러스터를 합침\n",
    "4) 유사도 행렬을 업데이트\n",
    "5) 하나의 클러스터가 남을 때까지 2~4번을 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f82f9-9c68-48bf-b9c7-1d4e095a823f",
   "metadata": {},
   "source": [
    "## 2. k-means\n",
    "\n",
    "k-means: 데이터를 정해진 개수로 나누되, 각 그룹의 중심점과의 거리가 가장 가까운 데이터끼리 묶는 군집화 알고리즘\n",
    "\n",
    "[k-means 알고리즘의 주요 단계]\n",
    "1) 데이터 표본들 중에서 랜덤하게 k개의 중심점을 초기 클러스터 중심으로 선택\n",
    "2) 각 표본들을 가장 가까운 중심점에 할당 (유클리디안 거리의 제곱을 사용)\n",
    "3) 각 클러스터에 할당된 표본들의 데이터 평균을 계산하여 중심점을 이동시킴\n",
    "4) 클러스터할당이 변하지 않거나 사용자가 지정한 허용 오차 또는 최대 반복 횟수에 도달할 때까지 2,3번을 반복\n",
    "\n",
    "** k-means 알고리즘은 원형 클러스터를 구분하는 데 뛰어나지만, 사전에 사용하자 클러스터의 개수를 직접 지정해줘야 한다는 단점이 있고, 이때 적절하지 않은 k를 고르면 군집 성능이 좋지 않아 k를 정할 때 주의가 필요함. (엘보우 방법과 실루엣 계수 등을 활용할 수 있음)\n",
    "\n",
    "[k-means 의 장단점]\n",
    "장점\n",
    "- 직관적이고 구현이 쉬움\n",
    "- 대용량 데이터에도 적용이 가능\n",
    "\n",
    "단점\n",
    "- 초기 중심값에 민감함: 초기 중심값을 설정하는 많은 방법이 존재하고, 그에 따라 형성되는 최종적인 군집이 달라질 수 있음.\n",
    "- 군집 수 결정이 어려움: 사용자가 직접 k값을 지정해야 하며, 이 k값이 결과의 질에 큰 영향을 미침.\n",
    "- 아웃라이어에 민감: 평균 중심으로 군집을 구성하기 때문에 아웃라이어 하나가 중심을 왜곡시킬 수 있음.\n",
    "- 기하학적인 모양의 군집은 파악하기 어려움. -> 이 문제의 해결을 위해 K-median, 데이터 포인트 하나를 선택하는 K-medoids, 밀도가 높은 곳으로 중심점을 이동하는 mean shift 알고리즘을 고려해 볼 수 있음.\n",
    "- 빈 클러스터 문제: 비어있는 클러스터가 존재할 수도 있는 문제가 있는데,,, 한 클러스터가 비어있다면 알고리즘이 빈 클러스터의 중심점에서 가장 멀리 떨어진 샘플을 찾고, 가장 먼 포인트에 중심점을 '다시' 할당하며 문제를 해결함!\n",
    "\n",
    "[k-means++ 알고리즘]\n",
    "k-means 에서는 초기 중심점을 랜덤하게 할당하기 때문에 초기 중심점에 따라 최종 군집화의 품질이 달라지거나 알고리즘 성능이 하락하는 단점이 있었음. 이를 해결하려면 k-means 알고리즘을 여러 번 실행해 SSE 입장에서 성능이 가장 좋은 모델을 선택하거나, k-means++ 알고리즘을 통해 '초기 중심점들을 서로 멀리 떨어진 곳에 위치'시키는 방법이 있음.\n",
    "\n",
    "[k-means++ 알고리즘의 초기화 과정]\n",
    "1) 선택한 k개의 중심점을 저장한 빈 집합 M을 초기화함.\n",
    "2) 입력 샘플에서 첫 번째 중심점을 랜덤하게 선택하고 M에 할당함.\n",
    "3) M에 없는 샘플 x에 대해 M에 있는 중심점까지으 ㅣ최소 제곱 거리를 찾음.\n",
    "4) 가중치가 적용된 확률 분포를 사용해 다음 중심점을 랜덤하게 선택함.\n",
    "5) k개의 중심점을 선택할 때까지 3, 4번을 반복함.\n",
    "6) 기본 k-means 알고리즘을 수행함.\n",
    "\n",
    "[엘보우 방법]\n",
    "k-means 알고리즘의 단점 중 하나는 적절한 k값에 따라 군집 품질이 달라진다는 것. 이를 해결하기 위해 엘보우 방법이라는 그래프를 활용해 최적 클러스터 개수 k개를 추정할 수 있음.\n",
    "-> 왜곡이 빠르게 감소하는 지점의 k값을 찾는 것!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c98223-c656-453f-87e2-c71e2b9ac72f",
   "metadata": {},
   "source": [
    "## 3. DBSCAN\n",
    "\n",
    "DBSCAN: 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘\n",
    "\n",
    "[DBSCAN 알고리즘의 주요 단계]\n",
    "1) 각 샘플을 핵심 샘플, 경계 샘플, 잡음 샘플 중 하나에 할당\n",
    "   - 어떤 샘플의 특정 반경 안에 있는 이웃 샘플이 지정된 개수 이상이면 핵심 샘플이 됨.\n",
    "   - 특정 반경 이내에 지정된 개수보다 이웃이 적지만 다른 핵심 샘플의 반경 안에 있으면 경계 샘플이 됨.\n",
    "   - 위 두 샘플에 해당하지 않는 다른 모든 샘플은 잡음 샘플.\n",
    "2) 개별 핵심 샘플 또는 핵심 샘플의 그룹을 클러스터로 만든다.\n",
    "3) 경계 샘플을 해당 핵심 샘플의 클러스터에 할당\n",
    "\n",
    "** 하이퍼 파라미터\n",
    "- eps: 데이터 포인트 사이의 최대 거리\n",
    "- min_samples: 포인트가 군집이 되기 위해 eps 거리 내 포인트가 얼마나 많아야 하는지 (min_samples 값이 증가하면 군집 수가 감소)\n",
    "- 어느 데이터 포인트를 기준으로 반경 eps 내에 기준이 되는 데이터 포인트를 포함하여 데이터 포인트의 개수가 min_samples 이상이면 하나의 군집으로 인식\n",
    "\n",
    "[DBSCAN의 장단점]\n",
    "장점\n",
    "- k-means처럼 클러스터 모양을 원형으로 가정하지 않고 밀도에 따라 클러스터를 할당 -> 임의의 기하학적 분포를 갖는 데이터도 잘 처리할 수 있음.\n",
    "- 이상치에 민감하지 않음 - noise를 통하여 outlier 검출 가능\n",
    "- k-means나 계층 군집과 달리 모든 샘플을 클러스터에 할당하지 않음 (잡음 샘플을 구분)\n",
    "\n",
    "단점\n",
    "- epsilon과 min_samples 설정에 많은 영향을 받음\n",
    "- epsilon을 조절하기 쉽지 않고, 적절한 조절을 위해서는 데이터셋에 대한 도메인 지식이 필요한 경우가 많음\n",
    "- 연산량이 많이 속도가 느림\n",
    "- 다른 밀도 분포를 가진 데이터의 군집 분석을 잘 하지 못함 -> 밀도가 높은 곳에 집중하기 때문에 밀도가 낮은 곳의 데이터를 하나의 군집으로 인식하지 못하고 noise point로 인식해버림\n",
    "- 차원의 저주를 벗어나지 못함 (모든 유클리디안 거리를 계산하는 알고리즘의 문제)\n",
    "\n",
    "** 차원의 저주: 같은 양의 데이터가 고차원으로 이동할수록 데이터포인트 사이의 거리가 늘어나 데이터의 밀도가 낮아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60e369-2a70-4419-b712-333f4cca6cfa",
   "metadata": {},
   "source": [
    "## 4. Gaussian Mixture Model (GMM)\n",
    "\n",
    "GMM: 데이터가 여러 다른 모양의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 군집화 방법\n",
    "GMM은 모델 기반 군집화 방법 중 하나로, 데이터를 생성하는 통계적 모델을 가정하고, 데이터가 해당 모델로부터 생성되었다는 전제 하에 군집화를 수행하는 접근 방식.\n",
    "\n",
    "이 방법은 각 군집을 확률 분포로 간주하고, 전체 데이터분포를 이질적인 여러 개의 확률 분포의 혼합으로 보고 모델링한다는 점에서 k-means와 같은 거리 기반 군집화와 차이가 있음. 데이터의 생성 메커니즘을 모델링할 수 있다는 특징이 있음.\n",
    "\n",
    "[GMM의 가정]\n",
    "1) 관측된 데이터포인트들은 특정 가우시안 확률 분포에 의해 생성되었다고 가정\n",
    "2) 군집화를 적용하고자 하는 전체 데이터셋에는 여러 개의 가우시안 분포가 섞여있고, 개별 데이터는 우도에 따라 k개의 가우시안 분포 중 한 가지에 속함. -> 섞인 데이터 분포에서 각각의 가우시안 분포를 추출해내고, 각각의 분포에 기반해 군집화를 수행\n",
    "\n",
    "** 확률과 우도\n",
    "1) 확률: 고정된 확률 분포에서 어떠한 관측값이 나타나는 지에 대한 확률\n",
    "2) 우도: 고정된 관측값이 어떠한 확률분포에서 어느정도의 확률로 나타나는지에 대한 확률\n",
    "\n",
    "[GMM의 진행과정]\n",
    "1) 주어진 전체 데이터셋의 분포를 확인\n",
    "2) 전체 데이터셋은 서로 다른 정규 분포 형태의 확률 분포 곡선으로 구성되어 있다고 가정\n",
    "3) 전체 데이터셋을 구성하는 여러 개의 정규분포 곡선을 추출, 개별 데이터가 이 중 어떤 정규분포에 속하는지 결정한다. 이때 각각의 분포가 하나의 군집이 됨.\n",
    "\n",
    "[모델의 파랄미터 추정]\n",
    "GMM에서는 기댓값-최대화 알고리즘(Expectation-Maximization, EM)을 사용하여 모델의 파라미터를 추정함. 이를 통해 각 군집의 중심, 모양, 크기, 방향까지 모델링 할 수 있음.\n",
    "\n",
    "EM 알고리즘의 파라미터 추정 과정\n",
    "1) initialization: 필요한 파라미터에 대해 초기값을 선정\n",
    "2) expection step: 현재 파라미터를 통해 x가 특정 분포(군집)에 속할 사후확률을 계산\n",
    "3) maximization step: 계산된 사후확률을 통해 파라미터를 다시 추정\n",
    "4) 수렴 조건이 만족될 때까지 반복한다.\n",
    "\n",
    "[GMM의 장단점]\n",
    "장점\n",
    "- K-means보다 유연하게 다양한 데이터 셋에 잘 적용될수 있음 (k-means가 잘 작동하지 않는 타원형 분포나, 중첩된 군집 구조에서도 좋은 성능을 낼 수 있음)\n",
    "\n",
    "단점\n",
    "- 군집화를 위한 수행시간이 오래 걸림\n",
    "- 가정한 분포에 맞지 않은 데이터일 경우 계산 복잡도가 높아지고 성능이 저하될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1d67de-f3db-4e37-b2ae-a13691e07e39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3269667571.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class sklearn.mixture.GaussianMixture(\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class sklearn.mixture.GaussianMixture(\n",
    "    n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1,\n",
    "    init_params='kmeans', weights_init=None, means_init=None, precsions_init=None, random_state=None,\n",
    "    warm_start=False, verbose=0, verbose_interval=10)[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebb2ce-22cc-4115-853c-b290eb21987f",
   "metadata": {},
   "source": [
    "# 3. 군집화의 평가 방법\n",
    "\n",
    "군집화는 비지도학습이기 때문에 알고리즘 자체의 내부 평가 지표를 사용해야 하며, k-means같은 경우에는 클래스 내 SSE를 사용한다. 내부 평가는 군집 내부의 응집도와 군집 간의 분리도를 정량적으로 측정하여 데이터 자체의 구조를 기반으로 군집 품질을 판단한다.\n",
    "\n",
    "데이터셋에 정답 레이블이 존재하는 경우에도 비지도 학습 방법을 적용하여 외부 평가 지표를 활용해 군집화 결과를 평가할 수도 있다. 외부 평가는 실제 정답이 존재하는 상황에서, 군집화 결과와 실제 클래스 간의 일치도를 비교함으로써 군집화의 정확성을 객관적으로 평가한다.\n",
    "\n",
    "1. 내부 평가\n",
    "- 정답 라벨이 없는 경우: 군집 내부의 응집도와 군집 간의 분리도를 기준으로 군집 품질을 평가함.\n",
    "- 대표적 지표: 실루엣 계수: 한 점이 자기 클러스터 내에서 얼마나 가깝고, 다른 클러스터와는 얼마나 멀리 떨어져 있는지 측정\n",
    "\n",
    "2. 외부 평가\n",
    "- 정답 라벨이 있는 경우, 실제 라벨과 군집화 결과를 비교할 수 있음 (ARI - adjusted rand index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391fc3c-5da7-4ca1-a944-5a0bfea47116",
   "metadata": {},
   "source": [
    "## 1) 실루엣 계수\n",
    "\n",
    "실루엣 계수: 클러스터 내 샘플들이 얼마나 조밀하게 모여 있는지를 측정하는 도구. 이를 통해 군집의 품질을 확인할 수 있고, -1과 1 사이의 값을 가진다.\n",
    "\n",
    "[계산 과정]\n",
    "1) 샘플 x와 동일한 클러스터 내 모든 다른 포인트 사이의 거리를 평균하여 클러스터 응집력을 계산\n",
    "2) 샘플 x와 가장 가까운 클러스터의 모든 샘플 간 평균 거리로 최근접 클러스터의 클러스터 분리도를 계산\n",
    "3) 클러스터 응집력과 분리도 사이의 차이를 둘 중 큰 값으로 나누어 실루엣을 계산\n",
    "\n",
    "클러스터 응집력과 분리도가 같은 경우 실루엣 계수는 0이 됨.\n",
    "클러스터 공식에서 b>>a임연 이상적인 실루엣 계수인 1에 가까워짐. b는 샘플이 다른 클러스터와 얼마나 다른지, a는 샘플이 클러스터 내 다른 샘플과 얼마나 비슷한지를 나타내기 때문\n",
    "\n",
    "sklearn 패키지를 통해 실루엣 계수를 계산할 수 있고, metric 모델 아래 silhouette_samples 함수를 이용하면 사용이 가능하며, silhouette_scores 함수를 ㅣ요할 수도 있음\n",
    "\n",
    "## 2) Dunn Index\n",
    "\n",
    "Dunn Index: 클러스터 간 최소 거리와 클러스터 내 최대 거리의 비율을 계산해 클러스터링의 품질을 평가하는 지표\n",
    "\n",
    "Dunn Index 값이 클수록 좋은 군집화 결과를 의미함. 기본적으로 군집화는 군집 간 거리가 크고 (분리도가 높고), 군집 내부 거리는 작을수록 (응집도가 높음) 좋기 때문임!\n",
    "\n",
    "한계\n",
    "- 군집 수가 많아질수록 계산 비용이 증가\n",
    "- 이상치에 민감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec203b-3311-475d-b0a5-11029b041851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
