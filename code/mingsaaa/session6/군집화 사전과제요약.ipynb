{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b8d436-aaed-4f45-ad70-b0d942468061",
   "metadata": {},
   "source": [
    "# 1️. 군집화(Clustering)\n",
    "\n",
    "## (1) 머신러닝: 비지도 학습\n",
    "\n",
    " **머신러닝** : 인공지능의 한 분야로, **컴퓨터가 스스로 학습**할 수 있도록 도와주는 알고리즘이나 기술을 개발하는 분야\n",
    "\n",
    "### 비지도 학습 (Unsupervised Learning)\n",
    "\n",
    "머신러닝은 크게 **지도학습(supervised learning)**과 **비지도학습(unsupervised learning)**으로 나뉩니다. \n",
    "비지도 학습에서는 정답이 없는 데이터를 이용하며, 패턴이 알려지지 않았거나 계속해서 변화하는 문제를 해결할 때 유용합니다.\n",
    "\n",
    "- 정답 레이블이 없기 때문에 **모델의 성능을 명확히 측정하기 어려움**\n",
    "- **표현 학습(representation learning)**을 수행하여 데이터셋의 고유 패턴을 식별할 수 있음\n",
    "\n",
    "## (2) 군집화란?\n",
    "\n",
    " **군집화(Clustering)** : 데이터를 **비슷한 특성을 가진 그룹(군집)**으로 나누는 비지도 학습 기법\n",
    "\n",
    "- **군집(cluster)** : 유사한 데이터들의 집합 (서로 다른 군집에 속한 개체들은 유사하지 않아야 함)\n",
    "- **유사성**을 기반으로 개체를 함께 그룹화하는 방법\n",
    "- 특정한 목표 없이 데이터 내 고유한 패턴을 발견하고 추출하는 도구\n",
    "\n",
    "### 군집화의 목표\n",
    "\n",
    "- **응집도(cohesion) 최대화** : 같은 군집에 속하는 데이터끼리는 최대한 비슷하도록 함\n",
    "- **분리도(separation) 최대화** : 서로 다른 군집은 최대한 분리되도록 함\n",
    "\n",
    " **데이터 간의 유사성을 유지하면서 서로 다른 그룹을 구분하는 것이 목표**\n",
    "\n",
    "## (3) 군집화 과정\n",
    "\n",
    "### 군집화의 기본 과정\n",
    "\n",
    "1. **피처 선택 또는 추출**\n",
    "2. **군집화 알고리즘 선택**\n",
    "3. **군집 유효성 검증**\n",
    "4. **결과 해석**\n",
    "\n",
    "### 군집화에서 고려해야 할 사항\n",
    "\n",
    "1. **변수 유형 이해**  \n",
    "   - 연속형 변수인지, 명목형 변수인지 파악\n",
    "   - 변수의 개수와 특징을 이해하여 적절한 방법론 선택\n",
    "\n",
    "2. **거리(또는 유사도) 정의와 측정**  \n",
    "   - 군집화는 데이터 간의 유사성을 기반으로 하기 때문에 **거리(distance) 또는 유사도(similarity)** 측정 방법이 중요\n",
    "   - 회귀 분석에서는 변수 자체가 중요하지만, 군집 분석에서는 **거리를 어떻게 정의하고 측정할 것인지**가 핵심\n",
    "\n",
    "3. **차원 축소**  \n",
    "   - 유사한 변수들을 묶어서 차원을 축소하여 분석의 효율성을 높임\n",
    "   - 예) 연봉, 나이, 연차, 직군 등의 변수가 있을 때, 나이와 연차를 비슷한 변수로 묶어 연차만 사용하는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ada8af-b736-4958-8c6d-b3332a0a5aad",
   "metadata": {},
   "source": [
    "# 2. 군집화 알고리즘\n",
    "\n",
    "---\n",
    "\n",
    "## (1) 계층적 군집화 (Hierarchical Clustering)\n",
    "\n",
    "### **계층적 군집화 개요**\n",
    "- 데이터 간의 유사성을 기반으로 트리 구조(dendrogram)를 형성하며, 상향식 또는 하향식 방식으로 군집을 형성하는 방법.\n",
    "- **군집 개수를 사전에 설정하지 않으며**, 클러스터링이 종료된 후 원하는 군집 개수를 선택.\n",
    "- 계층적 군집화 방식:\n",
    "  - **응집형 계층적 군집화 (Agglomerative Clustering)**: 작은 군집들을 병합하며 큰 군집을 형성.\n",
    "  - **분리형 계층적 군집화 (Divisive Clustering)**: 하나의 군집을 분할하며 여러 군집을 형성.\n",
    "- 결과는 일반적으로 **이진 트리(dendrogram)** 형태로 표현됨.\n",
    "\n",
    "### **응집형 계층적 군집화 (Agglomerative Clustering)**\n",
    "- 초기에는 모든 샘플이 각각 독립된 클러스터로 시작.\n",
    "- 가장 가까운 클러스터를 반복적으로 병합하여 하나의 클러스터로 만듦.\n",
    "- **군집 간 거리 측정 방법**에 따라 알고리즘이 달라짐.\n",
    "\n",
    "### **6가지 거리 정의 방법**\n",
    "1. **Single linkage (단일 연결법)**: 가장 가까운 두 점 사이의 거리로 군집 간 거리 측정 (이상치에 취약).\n",
    "2. **Complete linkage (완전 연결법)**: 가장 먼 두 점 사이의 거리로 군집 간 거리 측정 (내부 응집성 강조).\n",
    "3. **Average linkage (평균 연결법)**: 모든 항목 간 거리의 평균을 이용.\n",
    "4. **Centroid Method (중심 연결법)**: 클러스터의 중심점 간 거리 측정.\n",
    "5. **Median Method (중앙 연결법)**: 군집 내 모든 샘플의 중앙값을 이용한 거리 측정.\n",
    "6. **Ward's Method (Ward 연결법)**: SSE(오차제곱합)의 증가량이 가장 작은 방향으로 군집 병합.\n",
    "\n",
    "### **응집형 계층적 군집화 과정 (완전 연결 방식 예시)**\n",
    "1. 모든 샘플의 거리 행렬을 계산.\n",
    "2. 각 샘플을 단일 클러스터로 표현.\n",
    "3. 가장 비슷하지 않은 샘플 사이의 거리를 기준으로 가장 가까운 두 클러스터를 병합.\n",
    "4. 유사도 행렬 업데이트.\n",
    "5. 하나의 클러스터가 남을 때까지 2~4번을 반복.\n",
    "\n",
    "---\n",
    "\n",
    "## (2) k-means 군집화\n",
    "\n",
    "### **k-means 개요**\n",
    "- 데이터를 K개의 그룹으로 나누는 군집화 알고리즘.\n",
    "- 각 데이터 포인트를 **가장 가까운 중심점(centroid)에 할당**하여 군집을 형성.\n",
    "- 일반적으로 **유클리디안 거리의 제곱**을 이용해 거리 계산.\n",
    "\n",
    "### **k-means 알고리즘 과정**\n",
    "1. K개의 중심점(centroid)을 랜덤하게 초기화.\n",
    "2. 각 데이터 포인트를 가장 가까운 중심점에 할당.\n",
    "3. 각 클러스터 내 데이터 평균을 계산하여 중심점을 이동.\n",
    "4. 중심점이 더 이상 이동하지 않거나, 최대 반복 횟수에 도달할 때까지 2~3번 반복.\n",
    "\n",
    "### **k-means의 목적 함수 (SSE 최소화)**\n",
    "- 군집 내 **제곱 오차합(SSE, Sum of Squared Errors)**을 최소화하는 것이 목표.\n",
    "\n",
    "### **k-means의 장단점**\n",
    "#### 장점\n",
    "- 직관적이고 구현이 쉬움.\n",
    "- 대용량 데이터에도 적용 가능.\n",
    "\n",
    "#### 단점\n",
    "- **초기 중심점(centroid)에 민감** → 결과가 다르게 나올 수 있음.\n",
    "- **군집 개수(K)를 사전에 정해야 함**.\n",
    "- **이상치(Outlier)에 취약** → 중심이 왜곡될 가능성이 높음.\n",
    "- **비구형(Non-spherical) 군집을 잘 구분하지 못함**.\n",
    "\n",
    "### **k-means++ (초기 중심점 개선 기법)**\n",
    "- 기존 k-means의 단점을 보완하기 위해 **초기 중심점을 더 멀리 떨어진 곳에서 선택**하는 방법.\n",
    "- 보다 **일관적이고 최적에 가까운 군집화 결과**를 도출할 수 있음.\n",
    "\n",
    "#### k-means++ 초기화 과정\n",
    "1. 첫 번째 중심점을 랜덤하게 선택.\n",
    "2. 각 샘플의 **기존 중심점들과의 최소 거리**를 계산.\n",
    "3. 거리에 비례한 확률을 이용해 새로운 중심점 선택.\n",
    "4. K개의 중심점이 선택될 때까지 반복.\n",
    "5. 이후 일반 k-means 알고리즘 수행.\n",
    "\n",
    "### **엘보우 방법 (Elbow Method) - 최적 K 찾기**\n",
    "- 클러스터 개수 K를 결정하는 방법 중 하나.\n",
    "- K값을 변화시키며 **SSE 변화를 그래프로 시각화**하여 최적의 K를 찾음.\n",
    "- 그래프에서 **SSE 감소 속도가 급격히 완화되는 지점**을 선택."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6fc4-c557-4e86-8caa-f9ff9623bc3f",
   "metadata": {},
   "source": [
    "## (3) DBSCAN\n",
    "\n",
    "### **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "DBSCAN은 밀도가 높은 지역의 데이터를 하나의 군집으로 묶고, 밀도 기준을 만족하지 못하는 점은 군집에 포함시키지 않는 군집화 알고리즘이다.\n",
    "\n",
    "### **DBSCAN 알고리즘의 주요 단계**\n",
    "1. 각 샘플을 다음 세 가지로 분류한다.\n",
    "   - **핵심 샘플 (core point)**: 특정 반경 $\\epsilon$ 안에 있는 이웃 샘플이 지정된 개수 (MinPts) 이상인 경우.\n",
    "   - **경계 샘플 (border point)**: $\\epsilon$ 이내에 MinPts보다 이웃이 적지만, 다른 핵심 샘플의 반경 $\\epsilon$ 안에 있는 경우.\n",
    "   - **잡음 샘플 (noise point)**: 위 두 조건에 해당하지 않는 경우.\n",
    "2. 개별 핵심 샘플 또는 ($\\epsilon$ 이내에 있는 핵심 샘플을 연결한) 핵심 샘플의 그룹을 클러스터로 만든다.\n",
    "3. 경계 샘플을 해당 핵심 샘플의 클러스터에 할당한다.\n",
    "\n",
    "### **DBSCAN의 장단점**\n",
    "#### **장점**\n",
    "- k-means처럼 클러스터 모양을 원형으로 가정하지 않으며, 밀도에 따라 클러스터를 할당할 수 있다.\n",
    "- 이상치 (Outlier)에 민감하지 않으며, 잡음 샘플을 통해 검출 가능하다.\n",
    "- 모든 샘플을 클러스터에 할당하지 않아 잡음 샘플을 구분할 수 있다.\n",
    "\n",
    "#### **단점**\n",
    "- `epsilon`과 `min_samples` 설정에 따라 결과가 크게 달라진다.\n",
    "- `epsilon` 값을 적절하게 조정하기 어렵고, 도메인 지식이 필요한 경우가 많다.\n",
    "- 연산량이 많아 속도가 느릴 수 있다.\n",
    "- 밀도 차이가 큰 데이터에서는 성능이 저하될 수 있다.\n",
    "- 차원의 저주 (curse of dimensionality)의 영향을 받을 수 있다.\n",
    "\n",
    "---\n",
    "\n",
    "## (4) Gaussian Mixture Model (GMM)\n",
    "\n",
    "### **Gaussian Mixture Model (GMM)**\n",
    "GMM은 데이터가 여러 개의 가우시안 분포로 구성되었다고 가정하고, 각 분포를 클러스터로 인식하는 군집화 방법이다.\n",
    "\n",
    "### **GMM의 특징**\n",
    "- 모델 기반 군집화 (Model-based Clustering) 기법 중 하나.\n",
    "- 데이터를 생성하는 통계적 모델을 가정하고 군집화를 수행한다.\n",
    "- 개별 데이터가 여러 개의 확률 분포 중 하나에 속할 확률을 기반으로 군집을 형성한다.\n",
    "\n",
    "### **GMM의 가정**\n",
    "- 데이터는 여러 개의 가우시안 확률 분포로부터 생성되었다고 가정한다.\n",
    "- 전체 데이터셋을 구성하는 여러 개의 정규분포 곡선을 추출하고, 각각을 군집으로 간주한다.\n",
    "\n",
    "### **GMM의 진행 과정**\n",
    "1. 전체 데이터셋의 분포를 확인한다.\n",
    "2. 데이터셋이 여러 개의 정규 분포 형태의 확률 분포 곡선으로 구성되었다고 가정한다.\n",
    "3. 각 정규 분포를 추출하고, 개별 데이터가 특정 정규 분포에 속하는지 결정한다.\n",
    "4. EM (Expectation-Maximization) 알고리즘을 이용해 모델의 파라미터를 추정한다.\n",
    "   - **E-Step**: 현재의 파라미터를 사용하여 각 데이터가 특정 분포에 속할 확률을 계산한다.\n",
    "   - **M-Step**: 계산된 확률을 기반으로 가우시안 분포의 파라미터를 업데이트한다.\n",
    "   - 수렴 조건이 만족될 때까지 반복한다.\n",
    "\n",
    "### **GMM의 장단점**\n",
    "#### **장점**\n",
    "- k-means보다 유연하며 다양한 데이터셋에 적용 가능하다.\n",
    "- 타원형 분포나 중첩된 군집 구조에서도 좋은 성능을 낼 수 있다.\n",
    "\n",
    "#### **단점**\n",
    "- 군집화를 위한 수행 시간이 오래 걸린다.\n",
    "- 가우시안 분포를 가정하기 때문에, 데이터가 가정과 다를 경우 성능이 저하될 수 있다.\n",
    "\n",
    "### **K-means vs. GMM 비교**\n",
    "|  | **K-means (중심 기반)** | **GMM (확률 분포 기반)** |\n",
    "| --- | --- | --- |\n",
    "| 장점 | 개별 군집 내 데이터가 원형으로 흩어져 있을 때 효과적 | 다양한 데이터 분포에 유연하게 적용 가능 |\n",
    "| 단점 | 길쭉한 방향으로 데이터가 밀집한 경우 최적의 군집화가 어려움 | 수행 시간이 오래 걸리며, 정규 분포 가정이 필요함 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb690f-6b90-426e-a217-79caf9e6addb",
   "metadata": {},
   "source": [
    "# 3️. 군집화의 평가 방법\n",
    "\n",
    "비지도 학습에서는 정답(label)이 없기 때문에 지도 학습의 평가 방법을 그대로 적용할 수 없습니다. 따라서 **군집화의 품질을 평가하는 내부 평가 지표(Internal Evaluation)와 외부 평가 지표(External Evaluation)**를 사용합니다.\n",
    "\n",
    "## 1. 내부 평가 (Internal Evaluation)\n",
    "- 정답 라벨이 없는 경우 사용.\n",
    "- **군집 내부의 응집도(cohesion)와 군집 간의 분리도(separation)를 측정**하여 군집 품질을 평가.\n",
    "- 대표적인 지표:\n",
    "  - **실루엣 계수(Silhouette Coefficient)**: 클러스터 내 샘플들이 얼마나 밀집해 있는지를 측정.\n",
    "\n",
    "## 2. 외부 평가 (External Evaluation)\n",
    "- 정답 라벨이 있는 경우, 실제 라벨과 군집화 결과를 비교하여 평가.\n",
    "- 대표적인 지표:\n",
    "  - **ARI (Adjusted Rand Index)**: 두 군집 결과 간의 유사도를 측정.\n",
    "  - ARI 값이 1에 가까울수록 실제 라벨과 군집화 결과가 유사함.\n",
    "\n",
    "---\n",
    "\n",
    "## (1) 실루엣 계수 (Silhouette Coefficient)\n",
    "군집 내 샘플들이 얼마나 조밀하게 모여 있는지를 평가하는 지표.\n",
    "\n",
    "### **계산 방법**\n",
    "1. 샘플 $\\mathbf{x}^{(i)}$와 같은 클러스터 내 다른 포인트 간 평균 거리 **(클러스터 응집력, $a^{(i)}$)** 계산.\n",
    "2. $\\mathbf{x}^{(i)}$와 가장 가까운 다른 클러스터 내 모든 샘플 간 평균 거리 **(클러스터 분리도, $b^{(i)}$)** 계산.\n",
    "3. 실루엣 계수는 다음과 같이 계산:\n",
    "   $$\n",
    "   s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{\\max(a^{(i)}, b^{(i)})}\n",
    "   $$\n",
    "- 값 범위: **-1 ~ 1**  \n",
    "  - **1에 가까울수록** 군집화 품질이 높음.  \n",
    "  - **0에 가까울수록** 군집 내/외부 차이가 적음.  \n",
    "  - **-1에 가까울수록** 잘못된 군집화.\n",
    "\n",
    "---\n",
    "\n",
    "## (2) Dunn Index\n",
    "**군집 간 최소 거리(분리도)와 군집 내 최대 거리(응집도)의 비율**을 계산해 군집화 품질을 평가하는 지표.\n",
    "\n",
    "### **계산 방법**\n",
    "$$\n",
    "D = \\frac{\\min_{i \\neq j} d(C_i, C_j)}{\\max_k \\delta(C_k)}\n",
    "$$\n",
    "- $d(C_i, C_j)$ : 클러스터 $C_i$와 $C_j$ 사이의 거리 (클러스터 중심 거리 또는 최소 거리)\n",
    "- $\\delta(C_k)$ : 클러스터 $C_k$ 내 최대 거리 (해당 클러스터 내 가장 멀리 떨어진 두 점 간 거리)\n",
    "\n",
    "### **특징**\n",
    "- Dunn Index 값이 **클수록** 좋은 군집화 결과.\n",
    "- 높은 분리도와 낮은 응집도를 가질수록 좋은 군집.\n",
    "- **단점**: 계산 비용이 크고, 이상치에 민감함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a80415-762a-4d32-bb7d-cff8119611e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
